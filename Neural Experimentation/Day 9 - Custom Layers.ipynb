{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At some point we will have to implement our own custom layers. We can incorporate these in models\n",
    "class FlipSignLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    # This is activated by call\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return X * -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNet(torch.nn.Module):\n",
    "    def __init__(self, *dims):\n",
    "        super().__init__()\n",
    "        self._modules[\"first_layer\"] = torch.nn.Linear(dims[0], dims[1])\n",
    "        self._modules[\"second_layer\"] = torch.nn.ReLU()\n",
    "        self._modules[\"third_layer\"] = FlipSignLayer()\n",
    "        self._modules[\"fourth_layer\"] = torch.nn.Linear(dims[1], dims[2])\n",
    "        \n",
    "    def forward(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "        # Running each of modules upon input\n",
    "        inp = self._modules[\"first_layer\"](inp)\n",
    "        inp = self._modules[\"second_layer\"](inp)\n",
    "        inp = self._modules[\"third_layer\"](inp)\n",
    "        inp = self._modules[\"fourth_layer\"](inp)\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[29.3887, 23.0214, 22.8416, 29.6440, 24.3271, 24.4853, 25.8768, 26.1630,\n",
       "         25.6556, 22.2919, 24.3260, 30.1035, 27.8868, 31.3106, 29.4413, 22.6502,\n",
       "         26.3820, 28.3579, 24.5227, 27.9262, 27.3116, 29.0480, 24.6664, 23.6390,\n",
       "         26.8320, 27.0954, 34.8502, 28.8629, 31.3889, 25.1338, 27.6076, 21.0981,\n",
       "         29.1580, 27.5863, 30.0696, 27.9143, 32.5327, 28.1086, 32.4983, 29.7998,\n",
       "         26.0591, 25.7864, 33.3250, 27.2345, 34.3764, 29.6239, 28.1153, 28.2970,\n",
       "         31.9451, 22.5710, 32.2950, 28.4330, 31.8398, 23.6982, 26.0560, 28.7050,\n",
       "         26.4636, 25.4484, 28.2710, 24.5987, 27.4215, 32.0461, 30.6935, 34.0772,\n",
       "         28.5402, 22.4663, 28.7440, 26.8397, 25.7816, 26.6278, 26.5492, 24.0636,\n",
       "         27.6262, 28.7638, 25.9158, 31.8723, 24.3145, 24.2697, 22.7294, 24.8195,\n",
       "         24.2369, 28.9756, 29.3242, 31.1284, 35.2181, 22.5364, 31.3144, 30.2369,\n",
       "         30.4376, 31.5393, 31.0419, 29.7808, 32.6896, 24.9879, 27.3114, 25.9619,\n",
       "         28.7440, 32.6253, 27.2867, 27.7597, 28.0505, 29.5112, 30.7903, 31.1562,\n",
       "         30.1296, 25.3461, 25.9796, 32.0365, 21.4616, 28.0223, 31.0899, 23.0430,\n",
       "         30.0789, 22.0120, 23.6957, 26.9674, 31.1091, 30.9186, 21.8242, 23.9662]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(120, 7) * 12 # Values up to about 20\n",
    "# Linear fn with Gaussian noise\n",
    "labels = X @ torch.Tensor([i * 0.1 for i in range(7)]) + 15  + torch.normal(0, 0.3, size = (1, 120))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_random_nn(lr:float):\n",
    "    model = RandomNet(7, 5, 1)\n",
    "    model.apply(initialize_parameters) # Initializing parameters for all Linear layers\n",
    "    loss = torch.nn.MSELoss()\n",
    "    trainer = torch.optim.AdamW(model.parameters(), lr)\n",
    "    return model, trainer, loss\n",
    "\n",
    "def train_random_nn(X: torch.Tensor, labels: torch.Tensor, lr:float, epochs:int):\n",
    "    model, trainer, loss = build_random_nn(lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Initializing graident to 0\n",
    "        trainer.zero_grad()\n",
    "        cost = loss(model(X), labels)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"epoch: \", epoch, \", cost: \", cost)\n",
    "        cost.backward()\n",
    "        # Stepping along gradient and updating weights\n",
    "        trainer.step()\n",
    "    return model\n",
    "\n",
    "def initialize_parameters(layer: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Applicable layer level function that can be applied by a NN to initialize each Linear Layer with Xavier weights \n",
    "    + constant bias\n",
    "    \"\"\"\n",
    "    # Initializing all torch.nn.Linear layers with xavier variance maintaining weights + uniform constants\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        # Initializing from distribution used to maintain variance so as to avoid exploding and vanishing gradients\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        torch.nn.init.constant(layer.bias, 0.2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-e1eb1389709c>:30: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  torch.nn.init.constant(layer.bias, 0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , cost:  tensor(646.4657, grad_fn=<MseLossBackward>)\n",
      "epoch:  100 , cost:  tensor(28.1946, grad_fn=<MseLossBackward>)\n",
      "epoch:  200 , cost:  tensor(22.3361, grad_fn=<MseLossBackward>)\n",
      "epoch:  300 , cost:  tensor(18.1466, grad_fn=<MseLossBackward>)\n",
      "epoch:  400 , cost:  tensor(14.3716, grad_fn=<MseLossBackward>)\n",
      "epoch:  500 , cost:  tensor(10.7341, grad_fn=<MseLossBackward>)\n",
      "epoch:  600 , cost:  tensor(10.6406, grad_fn=<MseLossBackward>)\n",
      "epoch:  700 , cost:  tensor(10.6362, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = train_random_nn(X, labels, 0.05, 701) # Cost drastically improves even with flip sign layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.8525,  22.0420,   6.9948,  14.3416,  15.4060,  10.9594,  14.7379,\n",
       "          13.7233,  16.6976,  17.7019,   9.8036,  18.0956,  22.7790,  20.9690,\n",
       "          21.5403,   6.0199,  -5.5988,  39.7358,  11.9324,  39.0403,  17.7365,\n",
       "          17.6920,   5.4032,  25.6624,  10.3643,  30.1315,  22.2080,   4.5017,\n",
       "          22.9967,  29.4886,  14.6891,  14.5611,  19.2987,  15.3195,  30.2501,\n",
       "          16.9690,  21.3254,  10.1256,  30.0962,  27.4048,  15.1011,  -7.2394,\n",
       "           4.9946,  21.5219,  23.2505, -16.4606,  24.9778,  13.6068,   3.9219,\n",
       "          16.5412,  20.4664,  28.9199,  18.3090,  38.9800,   2.6729,  10.5015,\n",
       "           0.9381,  18.9293,   6.5587,   9.8564,   9.9447,  -0.6758,   8.0680,\n",
       "          10.7872,  25.1162,  15.1156,  22.1917,  -7.8005,  23.3381,  15.6603,\n",
       "          26.4831,  20.5113, -15.5249,  23.3799,  26.5649,  13.7049,   5.1545,\n",
       "          12.7119,  10.5934,  -2.1316,   5.2542,   1.6588,   2.9450,  17.9271,\n",
       "         -14.7203,  11.2312,  16.5431,  13.1107,  35.3003,   4.1700,  -3.5152,\n",
       "           6.6142,  31.0751,   2.2378,   6.7068,   8.8540,   4.4668,  -2.5183,\n",
       "          28.3309,  31.5664,  30.9372,  12.6880,  -5.3934,  38.1132,  15.9549,\n",
       "          38.3320,  10.1278,   2.8645,  19.5471,  -8.7211,  16.0121,  -1.8812,\n",
       "          14.4650,   6.4973,  18.5430,   8.4855,  16.3718,  -0.5166,  11.8448,\n",
       "           3.7222]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(120, 7) * 12 # Attempting same thing with negative labels\n",
    "# Linear fn with Gaussian noise\n",
    "labels = X @ torch.Tensor([i * 0.1 for i in range(7)]) + 15  + torch.normal(0, 0.3, size = (1, 120))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-e1eb1389709c>:30: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  torch.nn.init.constant(layer.bias, 0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , cost:  tensor(512.5497, grad_fn=<MseLossBackward>)\n",
      "epoch:  100 , cost:  tensor(147.1909, grad_fn=<MseLossBackward>)\n",
      "epoch:  200 , cost:  tensor(138.0045, grad_fn=<MseLossBackward>)\n",
      "epoch:  300 , cost:  tensor(137.8608, grad_fn=<MseLossBackward>)\n",
      "epoch:  400 , cost:  tensor(137.8588, grad_fn=<MseLossBackward>)\n",
      "epoch:  500 , cost:  tensor(137.8574, grad_fn=<MseLossBackward>)\n",
      "epoch:  600 , cost:  tensor(137.8563, grad_fn=<MseLossBackward>)\n",
      "epoch:  700 , cost:  tensor(137.8555, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = train_random_nn(X, labels, 0.05, 701) # Still gets much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson: Can use NN Modules to encode any arbitrary layer we'd like within the scope of a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Parametric Layer\n",
    "class MyLinearLayer(torch.nn.Module):\n",
    "    def __init__(self, *layer_dims):\n",
    "        super().__init__()\n",
    "        # Initializing weights\n",
    "        weight = torch.randn(layer_dims)\n",
    "        \n",
    "        # Defining properly dimensioned bias\n",
    "        bias = torch.zeros(layer_dims[-1])\n",
    "        \n",
    "        # Casting to a parameter\n",
    "        self.bias = torch.nn.Parameter(bias)\n",
    "        \n",
    "        # Initializing so as to maintain variance\n",
    "        torch.nn.init.xavier_uniform_(weight)\n",
    "        \n",
    "        # Casting to a parameter\n",
    "        self.weight = torch.nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        # Getting relu of matrix multiplication + bias\n",
    "        return torch.nn.functional.relu(data @ self.weight + self.bias) # Mapping to inputs by neuron space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomNet using custom linear layers\n",
    "class RandomNet2(torch.nn.Module):\n",
    "    def __init__(self, *dims):\n",
    "        super().__init__()\n",
    "        self._modules[\"first_layer\"] = MyLinearLayer(dims[0], dims[1])\n",
    "        self._modules[\"second_layer\"] = MyLinearLayer(dims[1], dims[2])\n",
    "    def forward(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "        # Running each of modules upon input\n",
    "        inp = self._modules[\"first_layer\"](inp)\n",
    "        inp = self._modules[\"second_layer\"](inp)\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_random_nn_2(lr:float):\n",
    "    model = RandomNet2(7, 5, 1)\n",
    "    loss = torch.nn.MSELoss()\n",
    "    trainer = torch.optim.AdamW(model.parameters(), lr)\n",
    "    return model, trainer, loss\n",
    "\n",
    "def train_random_nn_2(X: torch.Tensor, labels: torch.Tensor, lr:float, epochs:int):\n",
    "    model, trainer, loss = build_random_nn_2(lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Initializing graident to 0\n",
    "        trainer.zero_grad()\n",
    "        cost = loss(model(X), labels)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"epoch: \", epoch, \", cost: \", cost)\n",
    "        cost.backward()\n",
    "        # Stepping along gradient and updating weights\n",
    "        trainer.step()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , cost:  tensor(230.0297, grad_fn=<MseLossBackward>)\n",
      "epoch:  100 , cost:  tensor(142.0027, grad_fn=<MseLossBackward>)\n",
      "epoch:  200 , cost:  tensor(137.8823, grad_fn=<MseLossBackward>)\n",
      "epoch:  300 , cost:  tensor(137.8648, grad_fn=<MseLossBackward>)\n",
      "epoch:  400 , cost:  tensor(137.8574, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomNet2(\n",
       "  (first_layer): MyLinearLayer()\n",
       "  (second_layer): MyLinearLayer()\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_random_nn_2(X, labels, 0.05, 500) # Still achieves an identical cost! Remember to define all model attributes as parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7747],\n",
       "        [-0.3079]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, 'test-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7747],\n",
       "        [-0.3079]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('test-file') # PyTorch capable of saving intermediary tensors and loading them in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomNet(\n",
       "  (first_layer): Linear(in_features=7, out_features=5, bias=True)\n",
       "  (second_layer): ReLU()\n",
       "  (third_layer): FlipSignLayer()\n",
       "  (fourth_layer): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # Eval displays neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dict, 'random-nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('random-nn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomNet(\n",
       "  (first_layer): Linear(in_features=7, out_features=5, bias=True)\n",
       "  (second_layer): ReLU()\n",
       "  (third_layer): FlipSignLayer()\n",
       "  (fourth_layer): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # All Keys in State Dict loaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
