{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.ToTensor() # Automated data to tensor conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bugfix to disable ssl checking \n",
    "import ssl \n",
    "ssl._create_default_https_context = ssl._create_unverified_context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar_data(batch_s:int = 512, data_transform = None):\n",
    "    \"\"\"Loads CIFAR10 Data\"\"\"\n",
    "    \n",
    "    # Loading data\n",
    "    cifar_train = torchvision.datasets.CIFAR10(root = \"../data\", train = True, transform = data_transform, download = True)\n",
    "    # Defining DataLoader object\n",
    "    cifar_train_loader = data.DataLoader(cifar_train, batch_s, shuffle = True, num_workers = 2)\n",
    "    cifar_test = torchvision.datasets.CIFAR10(root = \"../data\", train = False, transform = data_transform, download = True)\n",
    "    cifar_test_loader = data.DataLoader(cifar_test, batch_s, shuffle = False, num_workers = 2)\n",
    "\n",
    "    return cifar_train_loader, cifar_test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train, cifar_test = load_cifar_data(512, data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Line numbers from now on for ease of use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have multiple input channels as in CIFAR, we can find different weighting across them to aggregate into a single output channel, or we can allow output channels to be maintained, using information from all 3 to form more \"detailed\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelConvNet(torch.nn.Module):\n",
    "    \"\"\"Defines MultiChannel Convolutional NN to be trained on CIFAR 10 label prediction\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3 dimensional convolutioal layer  - learning latent representations as useful to enumerate distinct useful features across channels and proceed to aggregate them\n",
    "        self._modules[\"first_layer\"] = torch.nn.Conv2d(3, 5, padding  = 2, kernel_size = (10,10), stride = 3) # 9x9x5\n",
    "        # Must specify padding for every dim\n",
    "        self._modules[\"second_layer\"] = torch.nn.Conv2d(5, 5, padding  = 2, kernel_size = (9,9)) # 5 * 5 * 5\n",
    "        self._modules[\"remaining_network\"] = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(125,45), torch.nn.ReLU(), torch.nn.Linear(45,15), torch.nn.ReLU(), torch.nn.Linear(15,10))\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Defining multichannel computation\n",
    "        X = torch.nn.functional.relu(self._modules[\"first_layer\"](X))\n",
    "        X = torch.nn.functional.relu(self._modules[\"second_layer\"](X))\n",
    "        X = self._modules[\"remaining_network\"](X)\n",
    "        return X\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying init to model to initialize all layer weights\n",
    "def build_convnet():\n",
    "    # Kernels are iterating across all three dimensions and all batch sizes simultaneously\n",
    "    model = MultiChannelConvNet()\n",
    "    trainer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    return model, trainer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, cost = build_convnet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiChannelConvNet(\n",
       "  (first_layer): Conv2d(3, 5, kernel_size=(10, 10), stride=(3, 3), padding=(2, 2))\n",
       "  (second_layer): Conv2d(5, 5, kernel_size=(9, 9), stride=(1, 1), padding=(2, 2))\n",
       "  (remaining_network): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=125, out_features=45, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=45, out_features=15, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=15, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # Initial mapping from 3 to 1 channels, then 1 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: torch.nn.Module, optimizer, cost, train_loader, epochs: int = 5):\n",
    "    for epoch in range(epochs):\n",
    "        # Using training loader reorganizes with channel first\n",
    "        for data, labels in train_loader: # Returns data, label tuple\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Obtaining cross entropy cost\n",
    "            loss = cost(model(data), labels)\n",
    "                \n",
    "            # Resetting gradient\n",
    "            # Computing gradients\n",
    "            loss.backward()\n",
    "            # Displaying cost every 10 iterations\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Printing end of epoch\n",
    "        print(\"cost: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  tensor(1.8996, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7116, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6793, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5457, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6389, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, cost, cifar_train, 5) # Training architecture for 5 epochs - reasonably improves on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D Kernels are used to compute localized aggregations of features across input channels, yielding aggregated \"channel feature\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting different architrecture\n",
    "class MultiChannelConvNet(torch.nn.Module):\n",
    "    \"\"\"Defines MultiChannel Convolutional NN to be trained on CIFAR 10 label prediction\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3 dimensional convolutioal layer  - learning latent representations as useful to enumerate distinct useful features across channels and proceed to aggregate them\n",
    "        self._modules[\"first_layer\"] = torch.nn.Conv2d(3, 5,padding = 2, kernel_size = (15,15), stride = 2) # 11* 11 **5\n",
    "        self._modules[\"second_layer\"] = torch.nn.Conv2d(5, 1, kernel_size = (1,1)) # 11*11*1\n",
    "        # Must specify padding for every dim\n",
    "        self._modules[\"third_layer\"] = torch.nn.Conv2d(1,1, padding  = 2, kernel_size = (16,16)) # 11 * 11\n",
    "        self._modules[\"remaining_network\"] = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(121,15), torch.nn.ReLU(), torch.nn.Linear(15,10))\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Defining multichannel computation\n",
    "        X = torch.nn.functional.relu(self._modules[\"first_layer\"](X))\n",
    "        X = torch.nn.functional.relu(self._modules[\"second_layer\"](X))\n",
    "        X = torch.nn.functional.relu(self._modules[\"third_layer\"](X))\n",
    "        X = self._modules[\"remaining_network\"](X)\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, cost = build_convnet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  tensor(2.3045, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.3025, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.3028, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.3027, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.3027, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, cost, cifar_train, 5) # Training architecture for 5 epochs - This architecture seems to perform far worse on CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, aggregating set of learned localized feature into a global question - goal of pool is to take all intermediary features within a receptive field and aggregate them into a single digit indicative of something. Coarser maps / aggregations = global representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample and aggregate, also lower sensativity to location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas a Kernel computes a stochastic weighting function, the pooling window is parameterless, performing deterministic aggregations across the training data\n",
    "\n",
    "A pxq pooling layer is a sliding window, computing the maxium or average of the subtensor in that specific position. \n",
    "\n",
    "This is positionally impartial as (with a max pool for example) no matter where a specific feature is, you will obtain the same output so long as it is within the locality window. Common intuition is images have tendency to deform merely because of movements of the camera, so insensitivity to minor shifts (likely the same deterministic aggregation so long as within a more general locality) should help recognize images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing empirically - remember to adjust stride as identical by default\n",
    "class MultiChannelConvNetPool(torch.nn.Module):\n",
    "    \"\"\"Defines MultiChannel Convolutional NN to be trained on CIFAR 10 label prediction\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3 dimensional convolutioal layer  - learning latent representations as useful to enumerate distinct useful features across channels and proceed to aggregate them\n",
    "        self._modules[\"first_layer\"] = torch.nn.Conv2d(3, 25, padding  = 2, kernel_size = (10,10), stride = 5) # 6x6x25\n",
    "        # Must specify padding for every dim\n",
    "        self._modules[\"second_layer\"] = torch.nn.Conv2d(25,30, padding =2, kernel_size = (5,5), stride = 1)\n",
    "        self._modules[\"third_layer\"] = torch.nn.MaxPool2d(2, padding =1, stride = 2) #4x4x35\n",
    "        # Skipping2, taking Maxpool of 3x3 Kernel window, returns 5x5x15\n",
    "        self._modules[\"remaining_network\"] = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(270,100), torch.nn.ReLU(), torch.nn.Linear(100,10))\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Defining multichannel computation\n",
    "        X = torch.nn.functional.relu(self._modules[\"first_layer\"](X))\n",
    "        X = torch.nn.MaxPool2d(2, padding=1, stride = 2)(X)\n",
    "        X = torch.nn.functional.relu(self._modules[\"second_layer\"](X))\n",
    "        # Not taking ReLU of what is already a linear linear transformation from ReLU space\n",
    "        X = self._modules[\"third_layer\"](X)\n",
    "        X = self._modules[\"remaining_network\"](X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying init to model to initialize all layer weights\n",
    "def build_convnet_pool():\n",
    "    # Kernels are iterating across all three dimensions and all batch sizes simultaneously\n",
    "    model = MultiChannelConvNetPool()\n",
    "    trainer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    return model, trainer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, cost = build_convnet_pool() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gilpasternak/Library/Python/3.8/lib/python/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  tensor(2.3044, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.3018, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2991, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2884, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2762, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.3035, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2580, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2733, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2601, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2348, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2052, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1713, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1312, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1115, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1246, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1669, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.4176, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.0640, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2219, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.2374, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1173, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1256, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1005, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1260, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1379, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1372, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1305, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.0821, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.0195, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.0309, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.0862, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.1087, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9994, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.0565, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.0163, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9588, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.0085, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9997, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9655, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9630, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9931, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9621, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(2.0286, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9564, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9106, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9117, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9961, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9011, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8987, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9059, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8906, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9214, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9145, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8154, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9456, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8330, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8282, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8347, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8609, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8302, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8462, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9261, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8861, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8804, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9624, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8920, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9007, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8597, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8687, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8231, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7604, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8363, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8622, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8041, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7595, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7796, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7477, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7882, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6698, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7738, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7046, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8043, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6859, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7321, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7330, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7964, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.9097, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7359, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7772, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7916, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7403, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7677, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7829, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7634, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6512, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8287, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.8931, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7084, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7752, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7142, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6847, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7245, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7741, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7741, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7473, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7399, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6872, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7113, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6351, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7380, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7822, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6489, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6688, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6729, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6692, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6747, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7034, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6872, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6629, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7032, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6704, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6111, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6473, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6355, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5773, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6115, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6230, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6366, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7176, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6615, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6269, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6809, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6171, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5720, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6463, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7087, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6988, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6829, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6366, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6564, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6687, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6140, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5955, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6795, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5924, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6253, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6407, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5494, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5888, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6531, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.7466, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6377, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6003, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6517, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6155, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5641, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6259, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6527, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6379, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6446, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5609, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6276, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5882, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5912, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6078, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5639, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5737, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5866, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6045, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6195, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  tensor(1.5767, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6137, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5826, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6311, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6458, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6605, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5853, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5641, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6554, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5308, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6439, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5829, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5494, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5187, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5198, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5412, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5475, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5207, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5423, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5250, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5057, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5767, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5243, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4963, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4364, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4488, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5576, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5001, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5837, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4604, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5933, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4636, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6307, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4572, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5749, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3936, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4577, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5053, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4888, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4312, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5307, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5946, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4680, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5645, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5563, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5530, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5335, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5533, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4514, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4442, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4381, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4017, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.6141, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5222, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4336, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4931, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4616, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5494, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4202, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5503, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4637, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4690, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4960, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3982, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5570, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5567, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5044, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5728, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4088, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4239, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5710, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5614, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4388, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3877, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4700, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4847, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5145, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5301, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5121, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5145, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4580, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5199, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4032, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4531, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4213, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5148, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4374, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4057, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4408, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4172, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4539, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4251, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4401, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4873, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4171, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4336, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3975, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4235, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4517, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4676, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4829, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3752, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4439, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3720, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3729, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4239, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3977, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3900, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4080, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4097, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4501, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3863, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4341, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4205, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4256, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4109, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5149, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3844, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3618, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4136, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4483, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4280, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3968, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4012, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3969, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4036, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4394, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3366, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4648, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4161, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3904, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3488, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3983, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4438, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3672, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4594, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4223, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4135, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3844, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3992, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3233, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3978, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4607, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4562, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4169, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5103, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  tensor(1.4071, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5186, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5234, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4675, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4620, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4300, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4234, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4238, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5726, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5074, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4532, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4098, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4357, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4348, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3852, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3721, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3715, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4236, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3833, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3652, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4567, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3991, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4010, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4214, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3991, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4679, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2687, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2964, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4158, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4485, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4021, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4369, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4292, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.5261, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3374, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4041, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3860, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4491, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4029, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4364, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4205, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3771, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3276, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3962, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3316, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3732, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3428, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3955, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4021, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2734, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3401, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2511, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3349, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3609, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3132, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4848, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3249, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4165, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2844, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4039, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3759, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2962, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3554, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3828, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3482, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3583, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3442, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3782, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3715, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4000, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4224, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3489, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2494, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4496, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4376, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4684, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3542, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4495, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4225, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3766, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2916, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4605, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3656, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3801, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4307, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3739, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3633, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3303, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3009, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2423, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3827, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3859, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3230, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2958, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4167, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3943, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3577, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2789, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4535, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3708, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3407, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3965, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3877, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3109, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3415, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4319, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3535, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4473, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3977, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3205, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3967, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4394, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2621, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2673, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4246, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4185, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4514, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4497, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4160, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3431, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3096, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3419, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3855, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3451, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3430, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4186, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4055, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3488, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3722, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3605, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3437, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3203, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3720, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3826, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3244, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2696, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2668, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3843, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2692, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.2987, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3764, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3353, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.4334, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3687, grad_fn=<NllLossBackward>)\n",
      "cost:  tensor(1.3334, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Model should - have deep featured latent space, reduce sensitivity to movement, learn at proper rate, be semi-deep\n",
    "train_model_pool(model, optimizer, cost, cifar_train, 5) # Training architecture for 5 epochs - Slight improvment by increasing latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying MaxPooling (computing defining features) and computing many latent features across the same localities helped the most! Pooling after every layer greatly helped improve stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple channels, maintain separation of localized aggregations irrespective of position across channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pool(model: torch.nn.Module, optimizer, cost, train_loader, epochs: int = 5):\n",
    "    for epoch in range(epochs):\n",
    "        # Using training loader reorganizes with channel first\n",
    "        for data, labels in train_loader: # Returns data, label tuple\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Obtaining cross entropy cost\n",
    "            loss = cost(model(data), labels)\n",
    "                \n",
    "            # Resetting gradient\n",
    "            # Computing gradients\n",
    "            loss.backward()\n",
    "            # Displaying cost every 10 iterations\n",
    "            optimizer.step()\n",
    "        \n",
    "            # Printing end of batch\n",
    "            print(\"cost: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexNet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
