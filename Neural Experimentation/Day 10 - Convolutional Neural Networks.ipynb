{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNets exploit natural image structure, reduce data in latent space without computationally intractable operations by aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spatial Invariance** - sweep an image by patches to learn representation of each subpatch of image, then aggregate a representation. (Aggregation of subspace).\n",
    "\n",
    "\n",
    "### 2 Major Criteria to make this suitable for Computer vision:\n",
    "\n",
    "\n",
    "**Translation Invariance** - Each Region should be initially treated indiscriminantly (we attempt to learn an identical thing about each part of the image initially).\n",
    "\n",
    "**Locality** - We focus on subregions of the image to begin with, without concern for how these subregions will affect other distant regions until later layers when the information is aggregated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteria Application in ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a ConvNet, we take a weighted function of pixles within a locality. We place the result of this output in the corresponding output subsection, AKA corner aggregation maintains corner status, thus maintaining the concept of spatiality.\n",
    "\n",
    "Translational Invariance is maintained by keeping the weight filter constant, such that no matter where we are in the image we are maintaining the same \"Question\" / learning an identical thing about each part of the image initially.\n",
    "\n",
    "Locality is maintained by having a convolutional filter which is less than the size of the image, thus aggregating information along various localities (subregions). AKA if v is the **kernel** / **filter** (this is an abstraction of a Kernel seen in Kernel Density, which is a weighting function for nearby points / features valuing them by distances, with a Neural Kernel instead valuing them by placement) then v > index i for some i will be 0 (far away features not weighted at all as do not impact local subimage understanding, just as window in KDE is capped by lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locality allows us to shrink our filters below the size of the image and learn local filters. Transitional invariance allows us to use one such filter uniformly across the image indiscriminantly. These together combine to the idea that we can learn a homogeneous feature about each locality, and by aggregating such homogenous questions can learn a valuable latent representation that can be used to classify an image. This methodology is extremely parameter efficient, however, as we only need to learn one weighting function per layer, drastically reducing the combinatorial parameterization of our models to a few hundred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it called a Convolution?\n",
    "\n",
    "Convolution is probability joint distribution adds up to something which is = to integral of a = x and b = z-x for across all x. This is equivalent to a sliding window across x in the discrete sense, because the operation in and of itself does not look at joint probability but does weigh possibilities in a convolutional fashion. In the 2 dim sense: x(a,b) * y(z-a, w-b) is a sliding window across the entirety of the [0,z]x[0,w] possibility space, attempting all combinatorial possibilities discretely. That is a rough discrete mapping to a convolution, where we aggregate all joint possibilities to understand the joint probability of an outcome of two variables. Also use one function to scale the other with an AND clause, just as is done in a convoluted multiplication\n",
    "\n",
    "In practice the mapping of the sliding filter is actually 3 dimensional to account for a third dimension: color in the RGB space\n",
    "\n",
    "Fundamental Operation: We convolve a weighting filter across an image to obtain an aggregation of several localities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions on Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operation is far closer to cross correlation (dot product) as aggregating how frequent our valuable features are. If intensity in a certain position is given a higher weight and it is more intense, then we will encode a greater number of logits of information (a valuable feature for classification present there, can map to ranges (37+ = presence of features => cat) This cross correlation is implemented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cor(t1: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Applies a kernel filter across a tensor, producing an aggregated latent representation\"\"\"\n",
    "    # New shape is how far short of endpoint filter must stop (because it cannot exceed boundaries of image) + 1 for the stopping iteration (if it is sized 2, it still aggregates when 2 away from edge)\n",
    "    new_shape = t1.shape[0] - kernel.shape[0] + 1, t1.shape[1] - kernel.shape[1] + 1\n",
    "    # A latent representation of aggregated localities\n",
    "    latent_representation = torch.rand(new_shape[0], new_shape[1]) # Using image dimensions\n",
    "    # Iterating and cross correlating\n",
    "    for i in range(new_shape[0]):\n",
    "        for j in range(new_shape[1]):\n",
    "            # Will dot tensors together via multiplication operator - this is the default\n",
    "            # Obtaining locality, using stride of 1, of sized kernel. Convoluted along\n",
    "            inter_tensor = t1[i:i+ kernel.shape[0], j:j+kernel.shape[1]]\n",
    "            # Obtaining cross correlation of locality with kernel (using kernel to weigh sum of information in locality)\n",
    "            weighted_locality_rep = (inter_tensor * kernel).sum() # Aggregating weighted feature information (reduces latent space rather than just weights)\n",
    "            latent_representation[i][j] = weighted_locality_rep\n",
    "    return latent_representation\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9023,  0.9184],\n",
       "        [ 0.3546,  1.3842],\n",
       "        [-0.4495, -1.7546]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_cor(torch.randn(4,3), torch.randn(2,2)) # Latent representation maintains aggregated features in localities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cor_4d(t1: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Applies a kernel filter across a tensor, producing an aggregated latent representation\"\"\"\n",
    "    # New shape is how far short of endpoint filter must stop (because it cannot exceed boundaries of image) + 1 for the stopping iteration (if it is sized 2, it still aggregates when 2 away from edge)\n",
    "    new_shape = t1.shape[0] - kernel.shape[0] + 1, t1.shape[1] - kernel.shape[1] + 1, t1.shape[2] - kernel.shape[2] + 1, t1.shape[3] - kernel.shape[3] \n",
    "    # A latent representation of aggregated localities\n",
    "    latent_representation = torch.rand(new_shape[0], new_shape[1], new_shape[2], new_shape[3]) # Using image dimensions\n",
    "    # Iterating and cross correlating\n",
    "    for i in range(new_shape[0]):\n",
    "        for j in range(new_shape[1]):\n",
    "            for k in range(t1.shape[1]):\n",
    "                # Will dot tensors together via multiplication operator - this is the default\n",
    "                # Obtaining locality, using stride of 1, of sized kernel. Convoluted along\n",
    "                inter_tensor = t1[i:i+ kernel.shape[0], j:j+kernel.shape[1], k:k+kernel.shape[2], 0:kernel.shape[3]]\n",
    "                # Obtaining cross correlation of locality with kernel (using kernel to weigh sum of information in locality)\n",
    "                weighted_locality_rep = (inter_tensor * kernel).sum() # Aggregating weighted feature information (reduces latent space rather than just weights)\n",
    "                latent_representation[i][j][k] = weighted_locality_rep\n",
    "    return latent_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining convolutional layer\n",
    "class Conv2D(torch.nn.Module):\n",
    "    def __init__(self, kernel_size: tuple):\n",
    "        super().__init__()\n",
    "        # These will be a learnable parameter. The NN will learn what it should consider important\n",
    "        self.weights = torch.nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        self.weights = torch.nn.init.xavier_uniform_(self.weights)\n",
    "        \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.relu(cross_cor(X, self.weights) + self.bias)\n",
    "\n",
    "    \n",
    "class Flatten(torch.nn.Module):\n",
    "    \"\"\"Non-Sequential flatten\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inp: torch.Tensor)->torch.Tensor:\n",
    "        temp = inp.detach().numpy().flatten()\n",
    "        new_tensor = torch.Tensor(temp)\n",
    "        new_tensor.requires_grad_ = True\n",
    "        return new_tensor\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConvNet(torch.nn.Module):\n",
    "    def __init__(self, *kernel_sizes):\n",
    "        super().__init__()\n",
    "        self._modules[\"layer_1\"] = Conv2D(kernel_sizes[0])\n",
    "        self._modules[\"layer_2\"] = Conv2D(kernel_sizes[1])\n",
    "        self._modules[\"layer_3\"] = Flatten()\n",
    "        self._modules[\"layer_4\"] = torch.nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor)-> torch.Tensor:\n",
    "        # Applying both convolutions\n",
    "        X = self._modules[\"layer_1\"](X)\n",
    "        X = self._modules[\"layer_2\"](X)\n",
    "        X = self._modules[\"layer_3\"](X)\n",
    "        X = self._modules[\"layer_4\"](X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from previous notebook\n",
    "# Converting to function for future use, default num_workers is 4 bc CPU threads\n",
    "def load_fashion_mnist(batch_size: int = 512, num_workers: int = 4):\n",
    "    data_transform = transforms.ToTensor() # Obtaining data to tensor converter\n",
    "    \n",
    "    # Downloading data\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root = \"../data\", train = True, transform = data_transform, download= True)  # Defining fashion MNIST train from torch datasets\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root = \"../data\", train = False, transform = data_transform, download = True)\n",
    "    \n",
    "    # Loading data onto an iterator\n",
    "    train_data_loader = data.DataLoader(mnist_train, batch_size, shuffle = True, num_workers = 4)\n",
    "    test_data_loader = data.DataLoader(mnist_test, batch_size, shuffle = True, num_workers = 4)\n",
    "    \n",
    "    # Returning iterator\n",
    "    return train_data_loader, test_data_loader \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_fashion_mnist(256, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying init to model to initialize all layer weights\n",
    "def build_model():\n",
    "    # Kernels are iterating across all three dimensions and all batch sizes simultaneously\n",
    "    model = BasicConvNet((15,15,1,1), (5,5,1,1))\n",
    "    trainer = torch.optim.Adam(model.parameters(), lr = 0.05, weight_decay=0.05)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    return model, trainer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, trainer, loss = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 4D code is merely an outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0810, -0.0288, -0.0281, -0.0854,  0.0934,  0.0305,  0.0277, -0.0560,\n",
       "         0.0600, -0.0218], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(28,28)) # 2 dimensional ConvNet Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
