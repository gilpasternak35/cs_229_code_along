{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNets exploit natural image structure, reduce data in latent space without computationally intractable operations by aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spatial Invariance** - sweep an image by patches to learn representation of each subpatch of image, then aggregate a representation. (Aggregation of subspace).\n",
    "\n",
    "\n",
    "### 2 Major Criteria to make this suitable for Computer vision:\n",
    "\n",
    "\n",
    "**Translation Invariance** - Each Region should be initially treated indiscriminantly (we attempt to learn an identical thing about each part of the image initially).\n",
    "\n",
    "**Locality** - We focus on subregions of the image to begin with, without concern for how these subregions will affect other distant regions until later layers when the information is aggregated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteria Application in ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a ConvNet, we take a weighted function of pixles within a locality. We place the result of this output in the corresponding output subsection, AKA corner aggregation maintains corner status, thus maintaining the concept of spatiality.\n",
    "\n",
    "Translational Invariance is maintained by keeping the weight filter constant, such that no matter where we are in the image we are maintaining the same \"Question\" / learning an identical thing about each part of the image initially.\n",
    "\n",
    "Locality is maintained by having a convolutional filter which is less than the size of the image, thus aggregating information along various localities (subregions). AKA if v is the **kernel** / **filter** (this is an abstraction of a Kernel seen in Kernel Density, which is a weighting function for nearby points / features valuing them by distances, with a Neural Kernel instead valuing them by placement) then v > index i for some i will be 0 (far away features not weighted at all as do not impact local subimage understanding, just as window in KDE is capped by lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locality allows us to shrink our filters below the size of the image and learn local filters. Transitional invariance allows us to use one such filter uniformly across the image indiscriminantly. These together combine to the idea that we can learn a homogeneous feature about each locality, and by aggregating such homogenous questions can learn a valuable latent representation that can be used to classify an image. This methodology is extremely parameter efficient, however, as we only need to learn one weighting function per layer, drastically reducing the combinatorial parameterization of our models to a few hundred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it called a Convolution?\n",
    "\n",
    "Convolution is probability joint distribution adds up to something which is = to integral of a = x and b = z-x for across all x. This is equivalent to a sliding window across x in the discrete sense, because the operation in and of itself does not look at joint probability but does weigh possibilities in a convolutional fashion. In the 2 dim sense: x(a,b) * y(z-a, w-b) is a sliding window across the entirety of the [0,z]x[0,w] possibility space, attempting all combinatorial possibilities discretely. That is a rough discrete mapping to a convolution, where we aggregate all joint possibilities to understand the joint probability of an outcome of two variables. Also use one function to scale the other with an AND clause, just as is done in a convoluted multiplication\n",
    "\n",
    "In practice the mapping of the sliding filter is actually 3 dimensional to account for a third dimension: color in the RGB space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
