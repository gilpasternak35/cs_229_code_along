{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with Linear Model: assumes a linear boundary could be drawn and assumes monotonicity: that a relationship between 2 variables stays consistent ad infintium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could define a linear layer on top of our current linear layer, but an affine function on top of an affine function is still affine, so we gain nothing. Our model is already currently capable of representing any affine function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given activations, we end up with linear combinations of non-linearities, forming complex non-linear decision boundaries. These become **Universal Approximators** - they can learn any function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiable non-linear functions that \"Activate\" a certain amount in some set space based on the specific value the input takes, but distorts scaling (not all inputs scaled evenly to outputs). All have gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000,\n",
       "        1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000, 4.5000, 5.0000,\n",
       "        5.5000, 6.0000, 6.5000, 7.0000, 7.5000])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLu Activation - max(0, input) - scaled inputs in the 0 to + inf space (all - inputs mapped to same value => - weights distorted + becomes different function)\n",
    "t1 = torch.arange(-8.0, 8.0, 0.5)\n",
    "torch.relu(t1) # Derivative 0 up until 0, then 1, not differentiable at 0\n",
    "# Amplifies positive activations, does not change negative so positive comparitively larger\n",
    "# Well behaved - lets argument through if positive so positive weights do not vanish\n",
    "# Problem: loses all negative information.\n",
    "# Does not learn negative scaling, just scales weight to 0\n",
    "# Adjusts positive weights towards optimum, negative weights to 0 (?)\n",
    "# Continues to adjust extreme weights, boosting confidence if even more extreme, does not adjust low confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3535e-04, 5.5278e-04, 9.1105e-04, 1.5012e-03, 2.4726e-03, 4.0701e-03,\n",
       "        6.6929e-03, 1.0987e-02, 1.7986e-02, 2.9312e-02, 4.7426e-02, 7.5858e-02,\n",
       "        1.1920e-01, 1.8243e-01, 2.6894e-01, 3.7754e-01, 5.0000e-01, 6.2246e-01,\n",
       "        7.3106e-01, 8.1757e-01, 8.8080e-01, 9.2414e-01, 9.5257e-01, 9.7069e-01,\n",
       "        9.8201e-01, 9.8901e-01, 9.9331e-01, 9.9593e-01, 9.9753e-01, 9.9850e-01,\n",
       "        9.9909e-01, 9.9945e-01])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(t1) # Sigmoid activation squashees to [0,1] space\n",
    "# Goal: break functional linearity\n",
    "# Problem: linear around 0, may adjusts small weights in a linear way such that linear boundary is not broken\n",
    "# Derivative close to 0 at extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9999, -0.9998,\n",
       "        -0.9993, -0.9982, -0.9951, -0.9866, -0.9640, -0.9051, -0.7616, -0.4621,\n",
       "         0.0000,  0.4621,  0.7616,  0.9051,  0.9640,  0.9866,  0.9951,  0.9982,\n",
       "         0.9993,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(t1) # Maps to the [-1,1] space, also squashes inputs so that \n",
    "# we obtain reasonable activations in linear combinations\n",
    "# Derivative is 0 at extremes - does not adjust extreme weights (extremely high/low confidence = rule in / out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from previous notebook\n",
    "# Converting to function for future use, default num_workers is 4 bc CPU threads\n",
    "def load_fashion_mnist(batch_size: int = 512, num_workers: int = 4):\n",
    "    data_transform = transforms.ToTensor() # Obtaining data to tensor converter\n",
    "    \n",
    "    # Downloading data\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root = \"../data\", train = True, transform = data_transform, download= True)  # Defining fashion MNIST train from torch datasets\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root = \"../data\", train = False, transform = data_transform, download = True)\n",
    "    \n",
    "    # Loading data onto an iterator\n",
    "    train_data_loader = data.DataLoader(mnist_train, batch_size, shuffle = True, num_workers = 4)\n",
    "    test_data_loader = data.DataLoader(mnist_test, batch_size, shuffle = True, num_workers = 4)\n",
    "    \n",
    "    # Returning iterator\n",
    "    return train_data_loader, test_data_loader \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_fashion_mnist(256, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f99e49b60d0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f99e488a910>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, test_loader # Verifying loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically choose layer widths in powers of 2, as these tend to be more hardware efficient (don't use uneccessary bit storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    zero_tensor = torch.zeros_like(X) # Defines a 0 tensor of identical shape to a given input\n",
    "    return torch.max(X, zero_tensor) # PyTorch distributes max comparisons elementwise across 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3.],\n",
       "        [1., 2., 3., 0.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(torch.Tensor([[-1, 1, 2, 3], [1,2,3,-4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(684.)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use @ for matrix multiplication\n",
    "t1 @ t1.T # Dot product using matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a multilayer model\n",
    "\n",
    "# Architecture: flatten input, map to 256 dimensions, break linearity having learned \n",
    "# 256 intermediate representations, in breaking linearity map all values to the positive space, keeping only poisitive activations\n",
    "# Lastly, use these features to reach 10 confidences which will be used to make a prediction.\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(X):\n",
    "    if isinstance(X, torch.nn.Linear): # should only initialize weights of linear layers\n",
    "        torch.nn.init.normal_(X.weight, mean = 0, std = 0.) #using pytorch to randomly initialize weights of that layer\n",
    "\n",
    "# Applying init to model to initialize all layer weights\n",
    "def build_model(input_dim:int, learn_rate:int, layer_dim, weight_decay_param:int = 0.09):\n",
    "    model = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(784, 64), torch.nn.ReLU(), torch.nn.Linear(64, 10), torch.nn.ReLU(), torch.nn.Softmax())\n",
    "    model.apply(init_weights)\n",
    "    trainer = torch.optim.Adam(model.parameters(), lr = learn_rate, weight_decay=weight_decay_param)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    return model, trainer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\"learn_rate\": 0.0005, \"input_dim\": 784, \"layer_dim\": [784, 256, 256, 10], \"num_epochs\": 20}\n",
    "model, trainer,loss = build_model(hyperparams[\"learn_rate\"], hyperparams[\"input_dim\"], hyperparams[\"layer_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sequential(\n",
       "   (0): Flatten(start_dim=1, end_dim=-1)\n",
       "   (1): Linear(in_features=784, out_features=64, bias=True)\n",
       "   (2): ReLU()\n",
       "   (3): Linear(in_features=64, out_features=10, bias=True)\n",
       "   (4): ReLU()\n",
       "   (5): Softmax(dim=None)\n",
       " ),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 784\n",
       "     weight_decay: 0.09\n",
       " ),\n",
       " CrossEntropyLoss())"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, trainer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gilpasternak/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost =  tensor(2.2632, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3487, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3153, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3466, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.2522, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3362, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3466, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3570, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3987, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3674, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.4091, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3466, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.2841, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3674, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.2632, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3778, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3362, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.1799, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.4091, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3362, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(hyperparams[\"num_epochs\"]):\n",
    "    for data, label in train_loader:\n",
    "        trainer.zero_grad() # resetting gradient to 0\n",
    "        cost = loss(model(data), label) # Computing cost\n",
    "        cost.backward() # Calculating gradients\n",
    "        trainer.step() # Stepping (backprop application)\n",
    "    print(\"cost = \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition for L2 regularization / weight decay :  distance of function from 0 == its complexity\n",
    "# Add penalty term to loss function which represents complexity (larger weights for single features)\n",
    "# Minimizing this will include a tendency to reduce weights such that function of weights minimized inside of loss\n",
    "    # Beneficial to reduce as weight reduction -> smaller loss\n",
    "# Regularization becomes L + lambda/2 * || w || ^2. \n",
    "# Reduces by derivation to lambda * sum(weights) derivative, which is a measure of the sum of complexity.\n",
    "# Lambda controls the fractional importance of error which we assign to complexity \n",
    "# (weights small => more variables with smaller weights => learns robust patterns => \n",
    "# more complex decision boundaries => fits better (penalty on larger terms))\n",
    "    # Gradient of L2 = mutlivar function with each variable scaled by lambda (reduce by lambda * var gradient = weight bc linear fn)\n",
    "    # => constant reduction from this part of function, larger if complexity more important (lambda) to reduce\n",
    "        # Lambda is sort of a weight decay parameter\n",
    "# Largest weights have the largest gradients and reduce more - better and more equivalent use of all information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost =  tensor(2.3043, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3100, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3135, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3089, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.2970, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.2898, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3012, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3095, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3088, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3056, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3061, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3049, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3033, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3107, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3070, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3011, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.2998, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.2981, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.2930, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3061, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Trying neural network on easier data\n",
    "def init_weights(X):\n",
    "    if isinstance(X, torch.nn.Linear): # should only initialize weights of linear layers\n",
    "        torch.nn.init.normal_(X.weight, mean = 0, std = 0.1) #using pytorch to randomly initialize weights of that layer\n",
    "\n",
    "# A generalized form\n",
    "def build_model(input_dim:int, learn_rate:int, layer_dims:list, weight_decay_param:int = 0.002):\n",
    "    \n",
    "    # Model with dropout added to ensure simpler feature fit\n",
    "    model = torch.nn.Sequential(torch.nn.Flatten(), \n",
    "                                torch.nn.Linear(layer_dims[0], layer_dims[1]), torch.nn.Sigmoid(),\n",
    "                                torch.nn.Linear(layer_dims[1], layer_dims[2]), \n",
    "                                torch.nn.ReLU(), torch.nn.Linear(layer_dims[2], layer_dims[3]), torch.nn.ReLU(), torch.nn.Softmax())\n",
    "    model.apply(init_weights)\n",
    "    trainer = torch.optim.Adam(model.parameters(), lr = learn_rate, weight_decay=weight_decay_param)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    return model, trainer, loss\n",
    "\n",
    "model, trainer,loss = build_model(hyperparams[\"learn_rate\"], hyperparams[\"input_dim\"], hyperparams[\"layer_dim\"])\n",
    "trainer.zero_grad()\n",
    "\n",
    "for i in range(hyperparams[\"num_epochs\"]):\n",
    "    for data, label in train_loader:\n",
    "        trainer.zero_grad() # resetting gradient to 0\n",
    "        cost = loss(model(data), label) # Computing cost\n",
    "        cost.backward() # Calculating gradients\n",
    "        trainer.step() # Stepping (backprop application)\n",
    "    print(\"cost = \", cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout and Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Models have **high bias** - \"decide in advance\" that they will represent only a specific type of relationship.\n",
    "\n",
    "Also have **low variance** - decision boundary pretty consistent based on random fluctuations in data (overall slope of line may change, but will not learn a completely different function. Will only learn a certain function with some fluctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks, on the other hand, have low bias as they can fit any function (no assumption) but high variance because they could fit a dataset so perfectly that a random fluctuation in data will completely change the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout introduced to fix high variance and tendency to build non generalizable,\n",
    "# extremely complex decision boundaries\n",
    "# Point of dropout is to train NN while breaking co-adaptation: next layers rely on previous \n",
    "# Ideally we'd like to generalize decision boundary by training a few neural nets with different architectures/data\n",
    "    # To account for fluctuation and then average them out\n",
    "# Stochastic fluctuations from training and population data balance out if we do this: get close to true\n",
    "    # Population distribution data-wise and a model that overfits in every different way ends up with the center of all overfits, which is likely a relatively smooth mid point\n",
    "        # The act of averaging is inherently an act of losing extreme patterns as they become smoother and more general: populations behave smoothly.\n",
    "# Units present with probability p at training time, always present at test time\n",
    "# At train: unit present with probability p, adds to prediction w*p to total prediction.\n",
    "# At test, always present with weight w*p, to account for expected contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop with probability p, retain with probability 1-p and divide by 1-p so that E(h) = 1-p *h/1-p = h.\n",
    "# Scales back to normal neural space where feature contributes equivalently to next activationa nd total prediction comprehensively\n",
    "# Allows a non-neglection of features in the feature space, and yet every prediction is a simpler boundary as it is reliant on fewer variables\n",
    "# AKA: we do not make any features less significant than others, but rather return to neural space by averaging a set of space-scaled simplistic neural predictions within it\n",
    "# Randomize so different model\n",
    "# More simplistic neural feature, but identical prediction space where otherwise it would be lesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What effect will backprop have?\n",
    "    # Theory 1: forces weights to increase as 0 weights may be unreliable if high weights are dropped out\n",
    "    # Theory 2: forces weights to become lower as learns simpler multi-variate patterns\n",
    "    # Theory 3: evenly weighs features such that high confidence complexities are backtracked\n",
    "# Conclusion - equivalent weight distributions, weights could be increased or decreased. Increased if need to use a remaining var to make prediction,\n",
    "    # Decreased if that var turns out to be suboptimal in the next dropout. Overall seems to lead to likely homogenous confidence AKA simpler patterns.\n",
    "    # Reduces complexity of patterns because balances out importance of variables, also reduces crucial feature search in training because feature needs to \n",
    "        # Be very crucial to persist in every dropout. However, trends pattern towards true crucial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2395, grad_fn=<SumBackward0>)\n",
      "tensor(1.1396, grad_fn=<SumBackward0>)\n",
      "tensor(0.0730, grad_fn=<SumBackward0>)\n",
      "tensor(0.0931, grad_fn=<SumBackward0>)\n",
      "tensor(0.3864, grad_fn=<SumBackward0>)\n",
      "tensor(0.2824, grad_fn=<SumBackward0>)\n",
      "tensor(0.3197, grad_fn=<SumBackward0>)\n",
      "tensor(0.1080, grad_fn=<SumBackward0>)\n",
      "tensor(0.1953, grad_fn=<SumBackward0>)\n",
      "tensor(0.0519, grad_fn=<SumBackward0>)\n",
      "tensor(0.0050, grad_fn=<SumBackward0>)\n",
      "tensor(0.0106, grad_fn=<SumBackward0>)\n",
      "tensor(0.0037, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<SumBackward0>)\n",
      "tensor(0.1318, grad_fn=<SumBackward0>)\n",
      "tensor(0.0201, grad_fn=<SumBackward0>)\n",
      "tensor(0.0916, grad_fn=<SumBackward0>)\n",
      "tensor(0.2036, grad_fn=<SumBackward0>)\n",
      "tensor(0.2473, grad_fn=<SumBackward0>)\n",
      "tensor(0.1187, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Experiment as to why things are failing\n",
    "\n",
    "# Trying neural network on easier data\n",
    "def init_weights(X):\n",
    "    if isinstance(X, torch.nn.Linear): # should only initialize weights of linear layers\n",
    "        torch.nn.init.normal_(X.weight, mean = 0, std = 0.1) #using pytorch to randomly initialize weights of that layer\n",
    "\n",
    "# A generalized form\n",
    "def build_model(input_dim:int, learn_rate:int, layer_dims:list, weight_decay_param:int = 0.002):\n",
    "    \n",
    "    # Model with dropout added to ensure simpler feature fit\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(layer_dims[0], layer_dims[1]), torch.nn.ReLU(), torch.nn.Dropout(0.2),\n",
    "                                torch.nn.Linear(layer_dims[1], layer_dims[2]), \n",
    "                                torch.nn.ReLU(), torch.nn.Linear(layer_dims[2], layer_dims[3]), torch.nn.ReLU(), torch.nn.Softmax())\n",
    "    model.apply(init_weights)\n",
    "    trainer = torch.optim.Adam(model.parameters(), lr = learn_rate, weight_decay=weight_decay_param)\n",
    "    loss = torch.nn.MSELoss()\n",
    "    return model, trainer, loss\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(784, 1), torch.nn.Linear(1,1))\n",
    "trainer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(hyperparams[\"num_epochs\"]):\n",
    "    ones = torch.ones(784) + torch.rand(784)\n",
    "    two = torch.Tensor([2])\n",
    "    trainer.zero_grad() # resetting gradient to 0\n",
    "    prediction = model(ones)\n",
    "    cost = loss(prediction, two) # Computing cost\n",
    "    cost.backward() # Calculating gradients\n",
    "    trainer.step() # Stepping (backprop application)\n",
    "    print(cost.sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
