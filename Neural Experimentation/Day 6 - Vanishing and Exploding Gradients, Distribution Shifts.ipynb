{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick trial with pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x\": [1,2,3,4,5, 3], \"y\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"e\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>y_a</th>\n",
       "      <th>y_b</th>\n",
       "      <th>y_c</th>\n",
       "      <th>y_d</th>\n",
       "      <th>y_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_1  x_2  x_3  x_4  x_5  y_a  y_b  y_c  y_d  y_e\n",
       "0    1    0    0    0    0    1    0    0    0    0\n",
       "1    0    1    0    0    0    0    1    0    0    0\n",
       "2    0    0    1    0    0    0    0    1    0    0\n",
       "3    0    0    0    1    0    0    0    0    1    0\n",
       "4    0    0    0    0    1    0    0    0    0    1\n",
       "5    0    0    1    0    0    0    0    0    0    1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df, columns = [\"x\", \"y\"]) # Turns each value in a categorical column into a dummy variable\n",
    "# Each value becomes a sparse vector with a 1 sentinel representing every time that value is seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation involves a multitiude of matrix multiplications, with the derivatives\n",
    "# Occasionally being positively correlated functions of the weights themselves\n",
    "# If weights too large: repeated multiplication of values with magnitude > 1 => exploding gradient => weights shifted radically to immediate divergence\n",
    "# Weights too small: repeated initial multiplication of values with magnitude < 1 => gradient of 0 => no optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of Sigmoid represents activations firing / not firing (very small intermediary)\n",
    "    # However, this curve also has 0 gradient for much of the function and a broad variety of inputs will produce \n",
    "    # minute gradients that vanish when multiplied by each other\n",
    "# ReLUs emerge as default choice for practitioners because good choice bc sustain gradients, not as direct at representing neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symmetry** - We seek to break symmetry as otherwise we would get homogeneous gradients across matrix multiplications, resulting in homogenous weighting functions across neurons, and thus obtaining the expressive power of only a single neuron (a combination of a few homogenous functions does not yield new features / weighting information, so only one feature of resultin g information is yielded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit assumption of independence: E(X) = E(w) *0 = 0, Var(X) = E(x^2) - E(x)^2 = E(x^2) = E(w^2) * E(x^2) = nin * sigma^2 * delta^2 (covariance is 0, so this is ninput features * variances)\n",
    "# If we fix variance of weights at one variance of weights will be variance of inputs by formula for variance and assuming expectation on inputs is 0\n",
    "# we can maintain variance of input to layer\n",
    "# By property of expectation variance of weight matrix n input features * sigma ^2\n",
    "# By same logic if we fix output as input to layer in backprop, by using identical logic we fix output to keep gradients constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xavier Initialization** - The idea is that we want to maintain variance in our data at a constant during forward and backpropagation so that our gradient neither vanishes nor explodes - AKA so that the Expectation of any function of our weights/outputs will be the variance of our initial inputs, being neither boosted nor squashed. If it is boosted, it could diverge. Otherwise, it could go to 0 as could the weights. Implicitly assumes that mean and inputs have variance 0 - an unpractical assumption. \n",
    "\n",
    "Math: Uses law of total probability to partition Expectation of outputs = Expectation of input * weight across all weights. Proceeds to claim that both of these expectations are 0 by implicit assumptions, hence expectation 0. Proceeds to use this to calculate variance of output, which is now just sum across inputs of E(Weight matrix)^2 * E(input)^2, since the means of all both of these are implicitly assumed to be drawn from dist of mean 0, both of these squared means are equivalent to variances, and we assume the variance of the input to be 1 since we cannot change it. \n",
    "\n",
    "We end up with n-inputs * variance of weights = 1 so that maintained at a constant. We want an identical property in backpropagation, the weight matrix is just a reverse mapping, and the input to backprop is output weights (or a function of them) so variance should also follow attribute of n-outputs * variance of weights = 1 (by independence). Hence variance of weights = 1/(ninputs + n-outputs), and we can use this to initialize uniform dist as variance is (b-a)^2 / 12, so we can equate and use to initialize distance between b and a. Since want uniform to have mean 0, this ends up being 2a^2/12  = -sqrt(6)/nin + nout and its absolute value.\n",
    "\n",
    "Xavier initialization uses these preset neural properties to sample from a distribution that naively should preserve variance in both forward propagation and backward propagation, with the average weight keeping the gradient adjustment from becoming too large or too small\n",
    "\n",
    "**Note**: We apply the Expectation * the # of elements before configuring squared expectation = variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
