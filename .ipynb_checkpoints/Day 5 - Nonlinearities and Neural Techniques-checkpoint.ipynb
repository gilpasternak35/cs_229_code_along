{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with Linear Model: assumes a linear boundary could be drawn and assumes monotonicity: that a relationship between 2 variables stays consistent ad infintium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could define a linear layer on top of our current linear layer, but an affine function on top of an affine function is still affine, so we gain nothing. Our model is already currently capable of representing any affine function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given activations, we end up with linear combinations of non-linearities, forming complex non-linear decision boundaries. These become **Universal Approximators** - they can learn any function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiable non-linear functions that \"Activate\" a certain amount in some set space based on the specific value the input takes, but distorts scaling (not all inputs scaled evenly to outputs). All have gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000,\n",
       "        1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000, 4.5000, 5.0000,\n",
       "        5.5000, 6.0000, 6.5000, 7.0000, 7.5000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLu Activation - max(0, input) - scaled inputs in the 0 to + inf space (all - inputs mapped to same value => - weights distorted + becomes different function)\n",
    "t1 = torch.arange(-8.0, 8.0, 0.5)\n",
    "torch.relu(t1) # Derivative 0 up until 0, then 1, not differentiable at 0\n",
    "# Amplifies positive activations, does not change negative so positive comparitively larger\n",
    "# Well behaved - lets argument through if positive so positive weights do not vanish\n",
    "# Problem: loses all negative information.\n",
    "# Does not learn negative scaling, just scales weight to 0\n",
    "# Adjusts positive weights towards optimum, negative weights to 0 (?)\n",
    "# Continues to adjust extreme weights, boosting confidence if even more extreme, does not adjust low confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3535e-04, 5.5278e-04, 9.1105e-04, 1.5012e-03, 2.4726e-03, 4.0701e-03,\n",
       "        6.6929e-03, 1.0987e-02, 1.7986e-02, 2.9312e-02, 4.7426e-02, 7.5858e-02,\n",
       "        1.1920e-01, 1.8243e-01, 2.6894e-01, 3.7754e-01, 5.0000e-01, 6.2246e-01,\n",
       "        7.3106e-01, 8.1757e-01, 8.8080e-01, 9.2414e-01, 9.5257e-01, 9.7069e-01,\n",
       "        9.8201e-01, 9.8901e-01, 9.9331e-01, 9.9593e-01, 9.9753e-01, 9.9850e-01,\n",
       "        9.9909e-01, 9.9945e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(t1) # Sigmoid activation squashees to [0,1] space\n",
    "# Goal: break functional linearity\n",
    "# Problem: linear around 0, may adjusts small weights in a linear way such that linear boundary is not broken\n",
    "# Derivative close to 0 at extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9999, -0.9998,\n",
       "        -0.9993, -0.9982, -0.9951, -0.9866, -0.9640, -0.9051, -0.7616, -0.4621,\n",
       "         0.0000,  0.4621,  0.7616,  0.9051,  0.9640,  0.9866,  0.9951,  0.9982,\n",
       "         0.9993,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(t1) # Maps to the [-1,1] space, also squashes inputs so that \n",
    "# we obtain reasonable activations in linear combinations\n",
    "# Derivative is 0 at extremes - does not adjust extreme weights (extremely high/low confidence = rule in / out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from previous notebook\n",
    "# Converting to function for future use, default num_workers is 4 bc CPU threads\n",
    "def load_fashion_mnist(batch_size: int = 512, num_workers: int = 4):\n",
    "    data_transform = transforms.ToTensor() # Obtaining data to tensor converter\n",
    "    \n",
    "    # Downloading data\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root = \"../data\", train = True, transform = data_transform, download= True)  # Defining fashion MNIST train from torch datasets\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root = \"../data\", train = False, transform = data_transform, download = True)\n",
    "    \n",
    "    # Loading data onto an iterator\n",
    "    train_data_loader = data.DataLoader(mnist_train, batch_size, shuffle = True, num_workers = 4)\n",
    "    test_data_loader = data.DataLoader(mnist_test, batch_size, shuffle = True, num_workers = 4)\n",
    "    \n",
    "    # Returning iterator\n",
    "    return train_data_loader, test_data_loader \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gilpasternak/Library/Python/3.8/lib/python/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_fashion_mnist(256, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7ffc10988ca0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7ffc10988af0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, test_loader # Verifying loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically choose layer widths in powers of 2, as these tend to be more hardware efficient (don't use uneccessary bit storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    zero_tensor = torch.zeros_like(X) # Defines a 0 tensor of identical shape to a given input\n",
    "    return torch.max(X, zero_tensor) # PyTorch distributes max comparisons elementwise across 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3.],\n",
       "        [1., 2., 3., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(torch.Tensor([[-1, 1, 2, 3], [1,2,3,-4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(684.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use @ for matrix multiplication\n",
    "t1 @ t1.T # Dot product using matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a multilayer model\n",
    "\n",
    "# Architecture: flatten input, map to 256 dimensions, break linearity having learned \n",
    "# 256 intermediate representations, in breaking linearity map all values to the positive space, keeping only poisitive activations\n",
    "# Lastly, use these features to reach 10 confidences which will be used to make a prediction.\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(X):\n",
    "    if isinstance(X, torch.nn.Linear): # should only initialize weights of linear layers\n",
    "        torch.nn.init.normal_(X.weight, mean = 0, std = 0.1) #using pytorch to randomly initialize weights of that layer\n",
    "\n",
    "# Applying init to model to initialize all layer weights\n",
    "def build_model(input_dim:int, learn_rate:int, layer_dim):\n",
    "    model = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(784, 64), torch.nn.ReLU(), torch.nn.Linear(64, 10), torch.nn.ReLU(), torch.nn.Softmax())\n",
    "    model.apply(init_weights)\n",
    "    trainer = torch.optim.Adam(model.parameters(), lr = learn_rate)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    return model, trainer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\"learn_rate\": 0.1, \"input_dim\": 784, \"layer_dim\": (256, 10), \"num_epochs\": 10}\n",
    "model, trainer,loss = build_model(hyperparams[\"learn_rate\"], hyperparams[\"input_dim\"], hyperparams[\"layer_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sequential(\n",
       "   (0): Flatten(start_dim=1, end_dim=-1)\n",
       "   (1): Linear(in_features=784, out_features=64, bias=True)\n",
       "   (2): ReLU()\n",
       "   (3): Linear(in_features=64, out_features=10, bias=True)\n",
       "   (4): ReLU()\n",
       "   (5): Softmax(dim=None)\n",
       " ),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 784\n",
       "     weight_decay: 0\n",
       " ),\n",
       " CrossEntropyLoss())"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, trainer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost =  tensor(2.3570, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3882, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3674, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3570, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3674, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3674, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3153, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3466, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3778, grad_fn=<NllLossBackward>)\n",
      "cost =  tensor(2.3674, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(hyperparams[\"num_epochs\"]):\n",
    "    for data, label in train_loader:\n",
    "        cost = loss(model(data), label) # Computing cost\n",
    "        trainer.zero_grad() # resetting gradient to 0\n",
    "        cost.backward() # Calculating gradients\n",
    "        trainer.step() # Stepping (backprop application)\n",
    "    print(\"cost = \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition for L2 regularization / weight decay :  distance of function from 0 == its complexity\n",
    "# Add penalty term to loss function which represents complexity (larger weights for single features)\n",
    "# Minimizing this will include a tendency to reduce weights such that function of weights minimized inside of loss\n",
    "    # Beneficial to reduce as weight reduction -> smaller loss\n",
    "# Regularization becomes L + lambda/2 * || w || ^2. \n",
    "# Reduces by derivation to lambda * sum(weights) derivative, which is a measure of the sum of complexity.\n",
    "# Lambda controls the fractional importance of error which we assign to complexity \n",
    "# (weights small => more variables with smaller weights => learns robust patterns => \n",
    "# more complex decision boundaries => fits better (penalty on larger terms))\n",
    "    # Gradient of L2 = mutlivar function with each variable scaled by lambda (reduce by lambda * var gradient = weight bc linear fn)\n",
    "    # => constant reduction from this part of function, larger if complexity more important (lambda) to reduce\n",
    "        # Lambda is sort of a weight decay parameter\n",
    "# Largest weights have the largest gradients and reduce more - better and more equivalent use of all information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
