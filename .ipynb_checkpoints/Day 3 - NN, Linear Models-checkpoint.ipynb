{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias == default when all featues take on value 0, basically offset/\n",
    "# Default prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors leads to order of magnitude level improvement in efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(w: torch.Tensor, b: torch.Tensor, num_examples: int):\n",
    "    normal_data = torch.normal(0,1, (num_examples, len(w))) # Generates data of a certain shape, either with a different mean and stddev for each element or a shared one for all elems\n",
    "    return normal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8263, -0.3378, -0.7590, -1.3058, -0.0880],\n",
       "        [ 0.8383, -1.7387, -0.6003,  0.3287,  0.9068],\n",
       "        [ 1.5445,  0.6526,  0.2522, -0.4596,  0.7441],\n",
       "        [ 0.6042,  0.8429,  1.4547,  0.6519,  1.7894],\n",
       "        [ 0.2583, -1.5769,  0.5673, -0.8607, -0.3156],\n",
       "        [-1.2678,  0.0126, -0.3155,  0.9588, -1.6057],\n",
       "        [ 0.2643,  0.9876,  0.3120, -0.3836,  1.3716],\n",
       "        [-0.5624, -0.1780, -0.3913,  1.0896,  0.0967],\n",
       "        [ 1.6429,  1.4594,  0.5817,  0.0041, -0.1786],\n",
       "        [ 0.0968, -0.5051, -2.5518, -1.1594,  2.5856],\n",
       "        [ 0.2620, -0.2029,  2.0110,  1.2230, -2.0159],\n",
       "        [ 0.3467, -1.5809, -0.0676,  0.7500,  0.8691],\n",
       "        [ 1.3077,  0.4531,  0.7718,  0.5086,  0.6707],\n",
       "        [-0.3690,  0.0495,  0.5064, -0.4081, -0.9435],\n",
       "        [ 0.0292, -0.2160, -0.1685, -1.4968,  0.2747]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data(torch.rand(5,3), torch.rand(1,3), 15) # Generate a bunch of Gaussian examples with mean 0 and stddev 1\n",
    "# This could be extremely useful as we could specify a different mean and stddev for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different mean and stddev for each feature:\n",
    "def generate_data_with_custom_dist(mean: torch.Tensor, stddev: torch.Tensor, w: torch.Tensor, b: torch.Tensor, num_examples: int):\n",
    "    normal_data = [torch.normal(mean,stddev) for i in range(num_examples)] # Generates data of a certain shape, either with a different mean and stddev for each element or a shared one for all elems\n",
    "    return normal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 2.0262,  5.6668, -0.0356,  5.6359,  2.0021]),\n",
       " tensor([ 1.3844,  1.6745, -0.0680,  5.7642, 12.3371]),\n",
       " tensor([ 0.8632, -0.1684,  2.3418,  2.9237,  9.4773]),\n",
       " tensor([ 0.3060,  0.8906,  1.3358,  0.6612, -6.3763]),\n",
       " tensor([0.7846, 3.5069, 2.3805, 7.4205, 5.5446]),\n",
       " tensor([ 1.8134,  1.5516,  5.5529,  1.3864, -1.7785]),\n",
       " tensor([0.5565, 0.2367, 2.7897, 6.8607, 8.0864]),\n",
       " tensor([2.8080, 5.6415, 5.4175, 2.0603, 2.9201]),\n",
       " tensor([0.9949, 0.3634, 0.3902, 1.2336, 6.2772]),\n",
       " tensor([ 2.0763,  3.6064,  4.3312,  4.7290, 17.3246]),\n",
       " tensor([1.4953, 0.1996, 1.7931, 6.3762, 6.0653]),\n",
       " tensor([3.1342, 3.6653, 7.9141, 5.7994, 2.7496]),\n",
       " tensor([ 0.1325,  2.0292, -1.5745,  3.4429,  5.6390]),\n",
       " tensor([0.8177, 2.2541, 7.6830, 9.4916, 3.5884]),\n",
       " tensor([2.7237, 2.1966, 6.7975, 2.3307, 3.7033])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate different features with different means and standard deviations\n",
    "generate_data_with_custom_dist(torch.Tensor([1,2,3,4,5]), torch.Tensor([1,2,3,4,5]), torch.rand(5,3), torch.rand(1,3), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a dataset with Gaussian noise\n",
    "def generate_dataset(mean: int = 1, \n",
    "                     stddev: int = 2, w: \n",
    "                     torch.Tensor = torch.rand((5,1)),\n",
    "                     b: torch.Tensor = torch.rand(1,1), \n",
    "                     num_examples: int = 50):\n",
    "    print(w.shape)\n",
    "    X = torch.normal(mean, stddev, (num_examples, len(w))) # Generates data of a certain shape, either with a different mean and stddev for each element or a shared one for all elems\n",
    "    y = torch.mm(X,w)\n",
    "    y = y + b\n",
    "    # Adding gaussian noise\n",
    "    y += torch.normal(0, 0.1, (num_examples,len(w[0])))\n",
    "    return X, y # Returning feature set, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7741],\n",
       "        [ 6.2064],\n",
       "        [ 9.5621],\n",
       "        [ 3.3655],\n",
       "        [14.0831],\n",
       "        [ 6.0961],\n",
       "        [ 9.0736]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = generate_dataset(5,7, torch.Tensor([[.1], [.1], [.7], [.05], [.05]]), torch.Tensor([2]), 7)# Labels generating deterministically + some noise with random weights\n",
    "# for a linearly regressive algorithm with some noise - linear relationship\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14.7194, -2.4425, -6.5926, 10.2956, -1.9313],\n",
       "        [ 4.6789, -2.9116,  5.3034, -1.5714, 10.0874],\n",
       "        [ 6.8084, 10.4848,  8.4068, -7.6185,  8.0880],\n",
       "        [ 2.3162, -0.7306,  0.7154, 14.5020, -1.0340],\n",
       "        [ 9.8748, -1.2867, 15.2288, 10.5760, -1.7192],\n",
       "        [14.4920,  1.8988,  3.8513, -2.4731, -3.7723],\n",
       "        [-1.0157,  2.0223,  9.4840,  1.5953,  8.5487]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# featues\n",
    "features # some randomized linear weighting of these + Gaussian noise yields results. To be more definitive, select weights beforehand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data generated above is Gaussian data generated with a linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[torch.randperm(features.shape[0])] # Using random permutation we shuffle the numbers up to a certain value, hence shuffling the rows / our training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.6789, -2.9116,  5.3034, -1.5714, 10.0874],\n",
       "        [ 2.3162, -0.7306,  0.7154, 14.5020, -1.0340],\n",
       "        [-1.0157,  2.0223,  9.4840,  1.5953,  8.5487],\n",
       "        [14.4920,  1.8988,  3.8513, -2.4731, -3.7723],\n",
       "        [14.7194, -2.4425, -6.5926, 10.2956, -1.9313],\n",
       "        [ 6.8084, 10.4848,  8.4068, -7.6185,  8.0880],\n",
       "        [ 9.8748, -1.2867, 15.2288, 10.5760, -1.7192]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning with a guess\n",
    "w = torch.normal(0, 1, (5,1), requires_grad = True) # A normal tensor autoinitialized which required a gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.6665],\n",
       "         [ 0.2049],\n",
       "         [ 0.6068],\n",
       "         [-0.6368],\n",
       "         [ 0.0205]], requires_grad=True),\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad = True) # Bias is generally initialized as 0\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definint basic linear regression model\n",
    "def linear_regression(X: torch.Tensor, w: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    # dimensions: examples * features x feature_weights(same dim as features) * 1 + 1*1 = \n",
    "    # examples * 1 (one weighted prediction applying all \n",
    "    # weights to each example (across column) to output a single prediction)\n",
    "    return torch.mm(X,w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.7213],\n",
       "        [13.6347],\n",
       "        [ 4.9106],\n",
       "        [10.5167],\n",
       "        [15.3698],\n",
       "        [10.7086],\n",
       "        [ 0.2856]], grad_fn=<AbsBackward>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(linear_regression(features, w, b) - labels) # a set of initial errors for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ((linear_regression(features,w, b) - labels)**2)  / len(labels) # A cost function (squared error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.sum().backward() # Backward propagating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating with gradient decent\n",
    "k = w - (0.005 * w.grad)\n",
    "w = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6582],\n",
       "        [-0.0061],\n",
       "        [ 0.3037],\n",
       "        [ 0.0583],\n",
       "        [-0.1611]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w # updated w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating b\n",
    "k = b - (0.005 * b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0075], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = k\n",
    "b # adjusted b parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.7731],\n",
       "        [-3.4413],\n",
       "        [-8.6390],\n",
       "        [ 7.8025],\n",
       "        [-5.4641],\n",
       "        [-0.8644],\n",
       "        [ 2.9600]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression(features, w, b) - labels # Overcompensated,but significantly lower cost given small number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Similar to d2l, it may be a good idea to build my own synthetic data generator for training models in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a neural network to solve this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = torch.nn.Sequential(torch.nn.Linear(5,1)) # Define a Sequential neural network model and pass it a linear layer\n",
    "# Layer maps from 5 features to 1 (weighting labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1063,  0.0761, -0.0834, -0.0849, -0.0536]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overriding weights in first layer of neural network (weights variable called 'weight')\n",
    "neural_network[0].weight.data.normal_(0, 0.1) # Returns randomly initialized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can only fill data of weight/bias\n",
    "neural_network[0].bias.data.fill_(0) # Underscore functions fill parameter (w/b) with some value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a loss function - Mean Squared Error\n",
    "cost = torch.nn.MSELoss()\n",
    "trainer = torch.optim.AdamW(neural_network.parameters(), lr = 0.03)# defining an optimizer: need to specify learning rate and our neural networks parameters to optimize over\n",
    "# nn.parameters() returns all parameters of neural networks to optimize over. We get back a trainer with an optimizer ready to optimize over params\n",
    "num_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  tensor(9.3138, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(9.0028, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.7795, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.7311, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.6163, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4990, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4603, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4930, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.5225, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.5075, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4633, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4267, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4217, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4432, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4653, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4677, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4502, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4258, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.4063, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3935, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3815, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3660, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3496, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3393, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3386, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3438, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3473, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3442, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3361, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3279, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3228, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3202, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3174, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3125, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3062, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.3003, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2951, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2900, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2843, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2784, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2737, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2706, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2684, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2661, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2626, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2584, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2541, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2500, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2461, grad_fn=<MseLossBackward>)\n",
      "cost:  tensor(8.2423, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Define model as a callable which returns predictions by default\n",
    "for epoch in range(num_epochs):\n",
    "    loss = cost(neural_network(features), labels) # Compute cost\n",
    "    print(\"cost: \", loss)\n",
    "    trainer.zero_grad() # Resets previous gradient\n",
    "    loss.backward() # Computes gradients\n",
    "    trainer.step()  # Updates parameters by gradient\n",
    "# Note: trainer keeps updates where it left off, MSE increases from 92 to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {Parameter containing:\n",
       "             tensor([[ 0.4241,  0.7119, -0.0726,  0.7008,  0.3963]], requires_grad=True): {'step': 50,\n",
       "              'exp_avg': tensor([[-0.2162, -0.0050, -0.0854,  0.0309, -0.0388]]),\n",
       "              'exp_avg_sq': tensor([[0.2851, 0.0438, 0.2171, 0.1787, 0.0529]])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.1588], requires_grad=True): {'step': 50,\n",
       "              'exp_avg': tensor([0.1990]),\n",
       "              'exp_avg_sq': tensor([0.0061])}})"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4241,  0.7119, -0.0726,  0.7008,  0.3963]]), tensor([-0.1588]))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network[0].weight.data,neural_network[0].bias.data #final predictions for weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
