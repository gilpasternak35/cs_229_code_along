{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating model for sake of parameters access\n",
    "model = torch.nn.Sequential(torch.nn.Linear(20, 2), torch.nn.ReLU(), torch.nn.Linear(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=20, out_features=2, bias=True), ReLU())"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0], model[1] # Accessing first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0157,  0.0521, -0.0347,  0.1497, -0.0891, -0.1076, -0.1529,  0.1386,\n",
       "                        0.0511, -0.0278, -0.1195,  0.0907, -0.1497,  0.1675,  0.0779, -0.0553,\n",
       "                        0.0068, -0.0675, -0.1974,  0.2047],\n",
       "                      [-0.0798,  0.1234, -0.0981, -0.0773, -0.1976,  0.0849,  0.1805, -0.0683,\n",
       "                       -0.1438,  0.1512,  0.1640, -0.2036,  0.1930, -0.1198, -0.0527, -0.1737,\n",
       "                       -0.0323, -0.1602,  0.1441,  0.1902]])),\n",
       "             ('bias', tensor([-0.1828, -0.0875]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].state_dict() # State dict returns layer weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0157,  0.0521, -0.0347,  0.1497, -0.0891, -0.1076, -0.1529,  0.1386,\n",
       "          0.0511, -0.0278, -0.1195,  0.0907, -0.1497,  0.1675,  0.0779, -0.0553,\n",
       "          0.0068, -0.0675, -0.1974,  0.2047],\n",
       "        [-0.0798,  0.1234, -0.0981, -0.0773, -0.1976,  0.0849,  0.1805, -0.0683,\n",
       "         -0.1438,  0.1512,  0.1640, -0.2036,  0.1930, -0.1198, -0.0527, -0.1737,\n",
       "         -0.0323, -0.1602,  0.1441,  0.1902]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All model parameters by default require gradient\n",
    "model[0].weight # Returns a Parameter object: parameter object stores state of parameter (information for gradients, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0157,  0.0521, -0.0347,  0.1497, -0.0891, -0.1076, -0.1529,  0.1386,\n",
       "          0.0511, -0.0278, -0.1195,  0.0907, -0.1497,  0.1675,  0.0779, -0.0553,\n",
       "          0.0068, -0.0675, -0.1974,  0.2047],\n",
       "        [-0.0798,  0.1234, -0.0981, -0.0773, -0.1976,  0.0849,  0.1805, -0.0683,\n",
       "         -0.1438,  0.1512,  0.1640, -0.2036,  0.1930, -0.1198, -0.0527, -0.1737,\n",
       "         -0.0323, -0.1602,  0.1441,  0.1902]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight.data # Parameter data attribute store tensor itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ifloordiv__',\n",
       " '__ilshift__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rmul__',\n",
       " '__rpow__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__torch_function__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_coalesced_',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_indices',\n",
       " '_is_view',\n",
       " '_make_subclass',\n",
       " '_nnz',\n",
       " '_reduce_ex_internal',\n",
       " '_to_sparse_csr',\n",
       " '_update_names',\n",
       " '_values',\n",
       " '_version',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'absolute',\n",
       " 'absolute_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'acosh',\n",
       " 'acosh_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'align_as',\n",
       " 'align_to',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'arccos',\n",
       " 'arccos_',\n",
       " 'arccosh',\n",
       " 'arccosh_',\n",
       " 'arcsin',\n",
       " 'arcsin_',\n",
       " 'arcsinh',\n",
       " 'arcsinh_',\n",
       " 'arctan',\n",
       " 'arctan_',\n",
       " 'arctanh',\n",
       " 'arctanh_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'as_subclass',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'asinh',\n",
       " 'asinh_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'atanh',\n",
       " 'atanh_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_and_',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bitwise_or',\n",
       " 'bitwise_or_',\n",
       " 'bitwise_xor',\n",
       " 'bitwise_xor_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'broadcast_to',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'cfloat',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clip',\n",
       " 'clip_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'col_indices',\n",
       " 'conj',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'copysign',\n",
       " 'copysign_',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'count_nonzero',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'crow_indices',\n",
       " 'cuda',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumprod_',\n",
       " 'cumsum',\n",
       " 'cumsum_',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'deg2rad',\n",
       " 'deg2rad_',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diff',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'divide',\n",
       " 'divide_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'exp2_',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'fix',\n",
       " 'fix_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float',\n",
       " 'float_power',\n",
       " 'float_power_',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'floor_divide',\n",
       " 'floor_divide_',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'frexp',\n",
       " 'gather',\n",
       " 'gcd',\n",
       " 'gcd_',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'greater',\n",
       " 'greater_',\n",
       " 'greater_equal',\n",
       " 'greater_equal_',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'has_names',\n",
       " 'heaviside',\n",
       " 'heaviside_',\n",
       " 'histc',\n",
       " 'hsplit',\n",
       " 'hypot',\n",
       " 'hypot_',\n",
       " 'i0',\n",
       " 'i0_',\n",
       " 'igamma',\n",
       " 'igamma_',\n",
       " 'igammac',\n",
       " 'igammac_',\n",
       " 'imag',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'inner',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_contiguous',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_leaf',\n",
       " 'is_meta',\n",
       " 'is_mkldnn',\n",
       " 'is_mlc',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'is_sparse_csr',\n",
       " 'is_vulkan',\n",
       " 'is_xpu',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'istft',\n",
       " 'item',\n",
       " 'kron',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'lcm',\n",
       " 'lcm_',\n",
       " 'ldexp',\n",
       " 'ldexp_',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'less',\n",
       " 'less_',\n",
       " 'less_equal',\n",
       " 'less_equal_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logcumsumexp',\n",
       " 'logdet',\n",
       " 'logical_and',\n",
       " 'logical_and_',\n",
       " 'logical_not',\n",
       " 'logical_not_',\n",
       " 'logical_or',\n",
       " 'logical_or_',\n",
       " 'logical_xor',\n",
       " 'logical_xor_',\n",
       " 'logit',\n",
       " 'logit_',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_exp',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'moveaxis',\n",
       " 'movedim',\n",
       " 'msort',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'multiply',\n",
       " 'multiply_',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'names',\n",
       " 'nan_to_num',\n",
       " 'nan_to_num_',\n",
       " 'nanmedian',\n",
       " 'nanquantile',\n",
       " 'nansum',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'negative',\n",
       " 'negative_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_empty_strided',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nextafter',\n",
       " 'nextafter_',\n",
       " 'nonzero',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'not_equal',\n",
       " 'not_equal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'outer',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'positive',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put',\n",
       " 'put_',\n",
       " 'q_per_channel_axis',\n",
       " 'q_per_channel_scales',\n",
       " 'q_per_channel_zero_points',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'quantile',\n",
       " 'rad2deg',\n",
       " 'rad2deg_',\n",
       " 'random_',\n",
       " 'ravel',\n",
       " 'real',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'refine_names',\n",
       " 'register_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'rename',\n",
       " 'rename_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'retain_grad',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'select',\n",
       " 'set_',\n",
       " 'sgn',\n",
       " 'sgn_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'signbit',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinc',\n",
       " 'sinc_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'square',\n",
       " 'square_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'subtract',\n",
       " 'subtract_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'swapaxes',\n",
       " 'swapaxes_',\n",
       " 'swapdims',\n",
       " 'swapdims_',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'take_along_dim',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tensor_split',\n",
       " 'tile',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_sparse',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'true_divide',\n",
       " 'true_divide_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unflatten',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsafe_chunk',\n",
       " 'unsafe_split',\n",
       " 'unsafe_split_with_sizes',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'values',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'vsplit',\n",
       " 'where',\n",
       " 'xlogy',\n",
       " 'xlogy_',\n",
       " 'xpu',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model[0].weight) # Massive object consisting of a lot of different data operations\n",
    "# Make sure to name layers something which is integer, as the layer keys have an attempted concat with the model repr and integer names will result in an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are hierarchially nested, as shown in example below\n",
    "def block1():\n",
    "    return torch.nn.Sequential(torch.nn.Linear(5,2), torch.nn.ReLU())\n",
    "\n",
    "def block2(input_net: torch.nn.Sequential):\n",
    "    net = torch.nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # Can add any module or block to a sequential NN (for a custom Neural Network, will have to add the modification to the execution flow ourselves)\n",
    "        # Adding named module to neural net\n",
    "        net.add_module(f'nested_block {i}', input_net)\n",
    "    \n",
    "    # Abstraction: instead of separate layer functionality we treat this identically to adding any Neural Block\n",
    "    net.add_module('final layer', torch.nn.Linear(2,1)) # First arg is a module as a modules added to the ._modules dict must be named\n",
    "    return net\n",
    " \n",
    "nested_model = torch.nn.Sequential(block2(block1())) \n",
    "nested_model.add_module('model_output', torch.nn.Softmax()) # Default name in modules dict is index in str form, but can add custom name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (nested_block 0): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=2, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (nested_block 1): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=2, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (nested_block 2): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=2, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (nested_block 3): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=2, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (final layer): Linear(in_features=2, out_features=1, bias=True)\n",
       "  )\n",
       "  (model_output): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_model # Model is hierarchial: executed sequentially, yet block structure maintained in parameter access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=2, bias=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_model[0][1][0] # Can obtain a specific layer by subindexing hierarchially\n",
    "# First sequential NN, second block, first layer = Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Softmax(dim=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_model[1] # Final layer of outermost model is 1 and can be accessed as such"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing NN from previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The use of torch.nn.Module is to define arbitrary architectures with control flow, one such attempted below:\n",
    "class NonSequentialNN(torch.nn.Module):\n",
    "    \"\"\"A NonSequential NN\"\"\"\n",
    "    def __init__(self, *dims):\n",
    "        \"\"\"Instantiates all layers with given dimensions\"\"\"\n",
    "        super().__init__() # Call to parent constructor\n",
    "        self.layers = [torch.nn.Linear, torch.nn.ReLU, torch.nn.Linear, torch.nn.ReLU, torch.nn.Dropout, torch.nn.Linear, torch.nn.ReLU]\n",
    "        k = -1 # Separate indexer to keep track of current layer\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            k += 1\n",
    "            # Activation and Dropout need not be initialized with dimensions\n",
    "            if self.layers[i] != torch.nn.Dropout and self.layers[i] != torch.nn.ReLU:\n",
    "                self._modules[str(i)] = self.layers[i](dims[k], dims[k+1])\n",
    "            else:\n",
    "                # If not a layer then functional assignment\n",
    "                self._modules[str(i)] = self.layers[i]()\n",
    "                k-=1\n",
    "            \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward propagation with control flow\"\"\"\n",
    "        for i in range(len(self._modules)):\n",
    "            X = self._modules[str(i)](X)\n",
    "        return X\n",
    "    \n",
    "def build_nonseq_nn(lr:float):\n",
    "    model = NonSequentialNN(7, 5, 3, 1)\n",
    "    model.apply(initialize_parameters) # Initializing parameters for all Linear layers\n",
    "    loss = torch.nn.MSELoss()\n",
    "    trainer = torch.optim.AdamW(model.parameters(), lr)\n",
    "    return model, trainer, loss\n",
    "\n",
    "def train_nonseq_nn(X: torch.Tensor, labels: torch.Tensor, lr:float, epochs:int):\n",
    "    model, trainer, loss = build_nonseq_nn(lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Initializing graident to 0\n",
    "        trainer.zero_grad()\n",
    "        cost = loss(model(X), labels)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"epoch: \", epoch, \", cost: \", cost)\n",
    "        cost.backward()\n",
    "        # Stepping along gradient and updating weights\n",
    "        trainer.step()\n",
    "    return model\n",
    "\n",
    "def initialize_parameters(layer: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Applicable layer level function that can be applied by a NN to initialize each Linear Layer with Xavier weights \n",
    "    + constant bias\n",
    "    \"\"\"\n",
    "    # Initializing all torch.nn.Linear layers with xavier variance maintaining weights + uniform constants\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        # Initializing from distribution used to maintain variance so as to avoid exploding and vanishing gradients\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        torch.nn.init.constant(layer.bias, 0.2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24.7618, 24.5149, 23.3156, 28.6803, 28.5994, 27.4192, 34.0341, 21.8394,\n",
       "         27.3328, 28.6167, 29.5413, 30.7348, 32.3054, 30.2460, 26.5944, 25.1440,\n",
       "         28.5584, 27.9645, 24.2513, 30.7692, 29.0718, 31.2432, 33.7478, 31.9613,\n",
       "         26.9352, 23.7830, 31.6831, 28.2437, 30.9579, 25.8252, 25.8471, 24.7871,\n",
       "         26.7193, 32.2231, 28.9167, 27.4429, 29.5729, 28.5647, 30.5918, 28.1465,\n",
       "         30.1989, 30.6434, 36.2119, 30.8035, 27.1705, 32.6814, 30.7025, 28.9919,\n",
       "         28.8582, 26.5958, 23.7464, 29.6078, 27.9791, 27.5476, 24.2762, 27.1946,\n",
       "         27.3742, 26.3235, 26.4626, 26.8952, 28.8788, 32.4711, 33.3996, 25.0261,\n",
       "         29.5017, 28.1793, 33.0077, 30.2728, 32.5918, 24.2421, 28.8471, 32.3291,\n",
       "         25.1473, 28.3657, 34.2052, 23.9359, 28.8678, 20.9647, 26.9096, 26.8324,\n",
       "         26.9209, 25.4982, 30.1717, 26.4333, 28.6541, 29.8631, 26.9784, 25.5272,\n",
       "         31.8813, 22.4839, 29.3475, 26.0065, 31.6725, 32.3125, 30.6244, 32.9005,\n",
       "         23.5230, 23.6957, 25.9560, 23.6042, 23.2903, 30.3738, 26.5254, 30.1832,\n",
       "         22.5258, 31.8209, 35.7171, 31.0888, 26.2665, 28.0783, 30.9621, 30.5506,\n",
       "         29.8373, 27.4518, 25.4685, 29.5013, 29.1840, 32.8347, 28.3709, 31.5323]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(120, 7) * 12 # Values up to about 20\n",
    "# Linear fn with Gaussian noise\n",
    "labels = X @ torch.Tensor([i * 0.1 for i in range(7)]) + 15  + torch.normal(0, 0.3, size = (1, 120))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-108-1ce72716fa07>:54: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  torch.nn.init.constant(layer.bias, 0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , cost:  tensor(612.8626, grad_fn=<MseLossBackward>)\n",
      "epoch:  100 , cost:  tensor(185.6455, grad_fn=<MseLossBackward>)\n",
      "epoch:  200 , cost:  tensor(92.9905, grad_fn=<MseLossBackward>)\n",
      "epoch:  300 , cost:  tensor(74.8376, grad_fn=<MseLossBackward>)\n",
      "epoch:  400 , cost:  tensor(59.4538, grad_fn=<MseLossBackward>)\n",
      "epoch:  500 , cost:  tensor(40.0095, grad_fn=<MseLossBackward>)\n",
      "epoch:  600 , cost:  tensor(35.1682, grad_fn=<MseLossBackward>)\n",
      "epoch:  700 , cost:  tensor(25.8079, grad_fn=<MseLossBackward>)\n",
      "epoch:  800 , cost:  tensor(22.0676, grad_fn=<MseLossBackward>)\n",
      "epoch:  900 , cost:  tensor(18.1518, grad_fn=<MseLossBackward>)\n",
      "epoch:  1000 , cost:  tensor(16.6361, grad_fn=<MseLossBackward>)\n",
      "epoch:  1100 , cost:  tensor(15.1271, grad_fn=<MseLossBackward>)\n",
      "epoch:  1200 , cost:  tensor(15.2100, grad_fn=<MseLossBackward>)\n",
      "epoch:  1300 , cost:  tensor(13.2052, grad_fn=<MseLossBackward>)\n",
      "epoch:  1400 , cost:  tensor(13.1474, grad_fn=<MseLossBackward>)\n",
      "epoch:  1500 , cost:  tensor(12.5100, grad_fn=<MseLossBackward>)\n",
      "epoch:  1600 , cost:  tensor(11.8993, grad_fn=<MseLossBackward>)\n",
      "epoch:  1700 , cost:  tensor(11.8939, grad_fn=<MseLossBackward>)\n",
      "epoch:  1800 , cost:  tensor(11.7029, grad_fn=<MseLossBackward>)\n",
      "epoch:  1900 , cost:  tensor(11.5665, grad_fn=<MseLossBackward>)\n",
      "epoch:  2000 , cost:  tensor(11.3199, grad_fn=<MseLossBackward>)\n",
      "epoch:  2100 , cost:  tensor(11.3134, grad_fn=<MseLossBackward>)\n",
      "epoch:  2200 , cost:  tensor(11.0875, grad_fn=<MseLossBackward>)\n",
      "epoch:  2300 , cost:  tensor(11.2175, grad_fn=<MseLossBackward>)\n",
      "epoch:  2400 , cost:  tensor(10.9753, grad_fn=<MseLossBackward>)\n",
      "epoch:  2500 , cost:  tensor(10.8363, grad_fn=<MseLossBackward>)\n",
      "epoch:  2600 , cost:  tensor(11.0222, grad_fn=<MseLossBackward>)\n",
      "epoch:  2700 , cost:  tensor(10.8653, grad_fn=<MseLossBackward>)\n",
      "epoch:  2800 , cost:  tensor(10.6947, grad_fn=<MseLossBackward>)\n",
      "epoch:  2900 , cost:  tensor(10.7098, grad_fn=<MseLossBackward>)\n",
      "epoch:  3000 , cost:  tensor(10.6356, grad_fn=<MseLossBackward>)\n",
      "epoch:  3100 , cost:  tensor(10.6328, grad_fn=<MseLossBackward>)\n",
      "epoch:  3200 , cost:  tensor(10.6314, grad_fn=<MseLossBackward>)\n",
      "epoch:  3300 , cost:  tensor(10.5425, grad_fn=<MseLossBackward>)\n",
      "epoch:  3400 , cost:  tensor(10.5351, grad_fn=<MseLossBackward>)\n",
      "epoch:  3500 , cost:  tensor(10.4513, grad_fn=<MseLossBackward>)\n",
      "epoch:  3600 , cost:  tensor(10.4803, grad_fn=<MseLossBackward>)\n",
      "epoch:  3700 , cost:  tensor(10.4797, grad_fn=<MseLossBackward>)\n",
      "epoch:  3800 , cost:  tensor(10.4675, grad_fn=<MseLossBackward>)\n",
      "epoch:  3900 , cost:  tensor(10.4210, grad_fn=<MseLossBackward>)\n",
      "epoch:  4000 , cost:  tensor(10.4170, grad_fn=<MseLossBackward>)\n",
      "epoch:  4100 , cost:  tensor(10.4068, grad_fn=<MseLossBackward>)\n",
      "epoch:  4200 , cost:  tensor(10.3904, grad_fn=<MseLossBackward>)\n",
      "epoch:  4300 , cost:  tensor(10.4028, grad_fn=<MseLossBackward>)\n",
      "epoch:  4400 , cost:  tensor(10.3739, grad_fn=<MseLossBackward>)\n",
      "epoch:  4500 , cost:  tensor(10.3620, grad_fn=<MseLossBackward>)\n",
      "epoch:  4600 , cost:  tensor(10.3654, grad_fn=<MseLossBackward>)\n",
      "epoch:  4700 , cost:  tensor(10.3461, grad_fn=<MseLossBackward>)\n",
      "epoch:  4800 , cost:  tensor(10.3459, grad_fn=<MseLossBackward>)\n",
      "epoch:  4900 , cost:  tensor(10.3293, grad_fn=<MseLossBackward>)\n",
      "epoch:  5000 , cost:  tensor(10.3460, grad_fn=<MseLossBackward>)\n",
      "epoch:  5100 , cost:  tensor(10.3246, grad_fn=<MseLossBackward>)\n",
      "epoch:  5200 , cost:  tensor(10.3246, grad_fn=<MseLossBackward>)\n",
      "epoch:  5300 , cost:  tensor(10.3169, grad_fn=<MseLossBackward>)\n",
      "epoch:  5400 , cost:  tensor(10.3194, grad_fn=<MseLossBackward>)\n",
      "epoch:  5500 , cost:  tensor(10.3205, grad_fn=<MseLossBackward>)\n",
      "epoch:  5600 , cost:  tensor(10.3131, grad_fn=<MseLossBackward>)\n",
      "epoch:  5700 , cost:  tensor(10.3048, grad_fn=<MseLossBackward>)\n",
      "epoch:  5800 , cost:  tensor(10.3039, grad_fn=<MseLossBackward>)\n",
      "epoch:  5900 , cost:  tensor(10.3057, grad_fn=<MseLossBackward>)\n",
      "epoch:  6000 , cost:  tensor(10.2974, grad_fn=<MseLossBackward>)\n",
      "epoch:  6100 , cost:  tensor(10.2974, grad_fn=<MseLossBackward>)\n",
      "epoch:  6200 , cost:  tensor(10.2948, grad_fn=<MseLossBackward>)\n",
      "epoch:  6300 , cost:  tensor(10.2933, grad_fn=<MseLossBackward>)\n",
      "epoch:  6400 , cost:  tensor(10.2915, grad_fn=<MseLossBackward>)\n",
      "epoch:  6500 , cost:  tensor(10.2895, grad_fn=<MseLossBackward>)\n",
      "epoch:  6600 , cost:  tensor(10.2888, grad_fn=<MseLossBackward>)\n",
      "epoch:  6700 , cost:  tensor(10.2873, grad_fn=<MseLossBackward>)\n",
      "epoch:  6800 , cost:  tensor(10.2871, grad_fn=<MseLossBackward>)\n",
      "epoch:  6900 , cost:  tensor(10.2866, grad_fn=<MseLossBackward>)\n",
      "epoch:  7000 , cost:  tensor(10.2878, grad_fn=<MseLossBackward>)\n",
      "epoch:  7100 , cost:  tensor(10.2834, grad_fn=<MseLossBackward>)\n",
      "epoch:  7200 , cost:  tensor(10.2846, grad_fn=<MseLossBackward>)\n",
      "epoch:  7300 , cost:  tensor(10.2837, grad_fn=<MseLossBackward>)\n",
      "epoch:  7400 , cost:  tensor(10.2816, grad_fn=<MseLossBackward>)\n",
      "epoch:  7500 , cost:  tensor(10.2826, grad_fn=<MseLossBackward>)\n",
      "epoch:  7600 , cost:  tensor(10.2828, grad_fn=<MseLossBackward>)\n",
      "epoch:  7700 , cost:  tensor(10.2809, grad_fn=<MseLossBackward>)\n",
      "epoch:  7800 , cost:  tensor(10.2810, grad_fn=<MseLossBackward>)\n",
      "epoch:  7900 , cost:  tensor(10.2804, grad_fn=<MseLossBackward>)\n",
      "epoch:  8000 , cost:  tensor(10.2798, grad_fn=<MseLossBackward>)\n",
      "epoch:  8100 , cost:  tensor(10.2798, grad_fn=<MseLossBackward>)\n",
      "epoch:  8200 , cost:  tensor(10.2799, grad_fn=<MseLossBackward>)\n",
      "epoch:  8300 , cost:  tensor(10.2791, grad_fn=<MseLossBackward>)\n",
      "epoch:  8400 , cost:  tensor(10.2789, grad_fn=<MseLossBackward>)\n",
      "epoch:  8500 , cost:  tensor(10.2789, grad_fn=<MseLossBackward>)\n",
      "epoch:  8600 , cost:  tensor(10.2786, grad_fn=<MseLossBackward>)\n",
      "epoch:  8700 , cost:  tensor(10.2783, grad_fn=<MseLossBackward>)\n",
      "epoch:  8800 , cost:  tensor(10.2783, grad_fn=<MseLossBackward>)\n",
      "epoch:  8900 , cost:  tensor(10.2782, grad_fn=<MseLossBackward>)\n",
      "epoch:  9000 , cost:  tensor(10.2777, grad_fn=<MseLossBackward>)\n",
      "epoch:  9100 , cost:  tensor(10.2778, grad_fn=<MseLossBackward>)\n",
      "epoch:  9200 , cost:  tensor(10.2776, grad_fn=<MseLossBackward>)\n",
      "epoch:  9300 , cost:  tensor(10.2775, grad_fn=<MseLossBackward>)\n",
      "epoch:  9400 , cost:  tensor(10.2774, grad_fn=<MseLossBackward>)\n",
      "epoch:  9500 , cost:  tensor(10.2773, grad_fn=<MseLossBackward>)\n",
      "epoch:  9600 , cost:  tensor(10.2774, grad_fn=<MseLossBackward>)\n",
      "epoch:  9700 , cost:  tensor(10.2774, grad_fn=<MseLossBackward>)\n",
      "epoch:  9800 , cost:  tensor(10.2770, grad_fn=<MseLossBackward>)\n",
      "epoch:  9900 , cost:  tensor(10.2770, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = train_nonseq_nn(X, labels, 0.05, 10000) # Being properly initialized, this model works very well! Variance maintained and gradients flow smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([27.0731], grad_fn=<AddBackward0>), tensor(29.))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor([1,2,3,4,5,6,7])), torch.Tensor([1,2,3,4,5,6,7]).dot(torch.Tensor([.1,.2,.3,.4,.5,.6,.7])) + 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not perfect, but not so far off!\n",
    "# Note: Neural Network seems to have a lot of trouble with understanding what a negative value is, much worse training performance when negatives included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
